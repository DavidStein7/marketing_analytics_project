{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emot in c:\\users\\ollin\\anaconda3\\lib\\site-packages (3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read csv file and name columns\n",
    "data = pd.read_csv(\"../Data/Twitter.csv\", header=None, encoding='latin-1')\n",
    "data.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date             user  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
       "\n",
       "                                               tweet  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the column 'query', as it only contains 'NO_QUERY'\n",
    "data = data.drop(columns= 'query')\n",
    "\n",
    "#Replace the 4 for a positive sentiment with a 1 for easier understanding (there are no numbers between 0 and 4)\n",
    "data['sentiment'] = data['sentiment'].replace(4, 1)\n",
    "#0 = negative, 1 = positive\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>time_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet                date  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t... 2009-04-06 22:19:45   \n",
       "1  is upset that he can't update his Facebook by ... 2009-04-06 22:19:49   \n",
       "2  @Kenichan I dived many times for the ball. Man... 2009-04-06 22:19:53   \n",
       "3    my whole body feels itchy and like its on fire  2009-04-06 22:19:57   \n",
       "4  @nationwideclass no, it's not behaving at all.... 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  hour time_group  \n",
       "0  2009      4        0  22:19:45    22      20-24  \n",
       "1  2009      4        0  22:19:49    22      20-24  \n",
       "2  2009      4        0  22:19:53    22      20-24  \n",
       "3  2009      4        0  22:19:57    22      20-24  \n",
       "4  2009      4        0  22:19:57    22      20-24  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert Date and time column into datetime format (By stripping day, month, year and time manually as strings and passing them into datetime function)\n",
    "data['date_new'] = data['date'].str[8:10] + \"/\" + data['date'].str[4:7] + \"/\" + data['date'].str[24:28] + \", \" + data['date'].str[11:19]\n",
    "data['date_new'] = pd.to_datetime(data['date_new'], format=\"%d/%b/%Y, %H:%M:%S\")\n",
    "\n",
    "#extract DateTime information from column\n",
    "data['year'] = pd.DatetimeIndex(data['date_new']).year\n",
    "data['month'] = pd.DatetimeIndex(data['date_new']).month\n",
    "\n",
    "#Weekday where 0 = Monday and 6 = Sunday\n",
    "data['weekday'] = pd.DatetimeIndex(data['date_new']).weekday\n",
    "data['time'] = pd.DatetimeIndex(data['date_new']).time\n",
    "data['hour'] = pd.DatetimeIndex(data['date_new']).hour\n",
    "\n",
    "# Extract Timezones\n",
    "data['date'] = data['date'].astype('string')\n",
    "data['timezone'] = data['date'].str[20:23]\n",
    "data.head()\n",
    "\n",
    "##sort time into groups\n",
    "#create list of conditions (time groups)\n",
    "conditions = [\n",
    "    (data['hour'] < 4),\n",
    "    (data['hour'] >= 4) & (data['hour'] < 8),\n",
    "    (data['hour'] >= 8) & (data['hour'] < 12),\n",
    "    (data['hour'] >= 12) & (data['hour'] < 16),\n",
    "    (data['hour'] >= 16) & (data['hour'] < 20),\n",
    "    (data['hour'] >= 20)\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['0-4', '4-8', '8-12', '12-16', '16-20', '20-24']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "data['time_group'] = np.select(conditions, values)\n",
    "\n",
    "#drop old date column\n",
    "data.drop(['date', 'timezone'], axis = 1, inplace=True)\n",
    "data.rename(columns={'date_new': 'date'}, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet                date  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t... 2009-04-06 22:19:45   \n",
       "1  is upset that he can't update his Facebook by ... 2009-04-06 22:19:49   \n",
       "2  @Kenichan I dived many times for the ball. Man... 2009-04-06 22:19:53   \n",
       "3    my whole body feels itchy and like its on fire  2009-04-06 22:19:57   \n",
       "4  @nationwideclass no, it's not behaving at all.... 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  hour time_group  word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45    22      20-24          19        False   \n",
       "1  2009      4        0  22:19:49    22      20-24          21         True   \n",
       "2  2009      4        0  22:19:53    22      20-24          18        False   \n",
       "3  2009      4        0  22:19:57    22      20-24          10        False   \n",
       "4  2009      4        0  22:19:57    22      20-24          21        False   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol   link  money  paragraph_symbol  \\\n",
       "0             False          False       True   True  False             False   \n",
       "1              True          False      False  False  False             False   \n",
       "2             False          False       True  False  False             False   \n",
       "3             False          False      False  False  False             False   \n",
       "4             False           True       True  False  False             False   \n",
       "\n",
       "   hashtag  \n",
       "0    False  \n",
       "1    False  \n",
       "2    False  \n",
       "3    False  \n",
       "4    False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the number of words per tweet\n",
    "data['word_count'] = data['tweet'].str.split().str.len()\n",
    "\n",
    "#Check, if certain special characters occur in a tweet (one-hot encoded)\n",
    "data['dot_dot_dot'] = data['tweet'].str.contains('\\.\\.\\.')\n",
    "data['exclamation_mark'] = data['tweet'].str.contains('!')\n",
    "data['question_mark'] = data['tweet'].str.contains('\\?')\n",
    "data['at_symbol'] = data['tweet'].str.contains('\\@')\n",
    "data['link'] = data['tweet'].str.contains('http')\n",
    "data['money'] = data['tweet'].str.contains('\\$|\\€|\\£')\n",
    "data['paragraph_symbol'] = data['tweet'].str.contains('\\§')\n",
    "data['hashtag'] = data['tweet'].str.contains('#')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {'dot_dot_dot': 'int64',\n",
    "                'exclamation_mark': 'int64',\n",
    "                'question_mark': 'int64',\n",
    "                'at_symbol': 'int64',\n",
    "                'link': 'int64',\n",
    "                'money': 'int64',\n",
    "                'paragraph_symbol': 'int64',\n",
    "                'hashtag': 'int64'\n",
    "                }\n",
    "data = data.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english') #Create a list of english stopwords from nltk\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_clean = [word.replace(\"'\", \"\") for word in stopword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the typical pattern of links ('https://', 'http://', 'www.'), tags ('@') and hashtags ('#')\n",
    "url_pattern_1 = r'https?://\\S+'\n",
    "url_pattern_2 = r'www\\.\\S+'\n",
    "tag_pattern = r'@\\S+'\n",
    "hashtag_pattern = r'#\\S+'\n",
    "\n",
    "#Add a new column for the tokenized tweet and remove all links, tags and hashtags from the tweets\n",
    "data.insert(4, 'tweet_tokenized', data['tweet'].apply(lambda x: re.sub(url_pattern_1, '', x)))\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: re.sub(url_pattern_2, '', x))\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: re.sub(tag_pattern, '', x))\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: re.sub(hashtag_pattern, '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':‑)': 'Happy face or smiley', ':-))': 'Very happy', ':-)))': 'Very very Happy face or smiley', ':)': 'Happy face or smiley', ':))': 'Very Happy face or smiley', ':)))': 'Very very Happy face or smiley', ':-]': 'Happy face or smiley', ':]': 'Happy face or smiley', ':-3': 'Happy face smiley', ':3': 'Happy face smiley', ':->': 'Happy face smiley', ':>': 'Happy face smiley', '8-)': 'Happy face smiley', ':o)': 'Happy face smiley', ':-}': 'Happy face smiley', ':}': 'Happy face smiley', ':-)': 'Happy face smiley', ':c)': 'Happy face smiley', ':^)': 'Happy face smiley', '=]': 'Happy face smiley', '=)': 'Happy face smiley', ':‑D': 'Laughing, big grin or laugh with glasses', ':D': 'Laughing, big grin or laugh with glasses', '8‑D': 'Laughing, big grin or laugh with glasses', '8D': 'Laughing, big grin or laugh with glasses', 'X‑D': 'Laughing, big grin or laugh with glasses', 'XD': 'Laughing, big grin or laugh with glasses', '=D': 'Laughing, big grin or laugh with glasses', '=3': 'Laughing, big grin or laugh with glasses', 'B^D': 'Laughing, big grin or laugh with glasses', ':-(': 'Frown, sad, andry or pouting', ':‑(': 'Frown, sad, andry or pouting', ':(': 'Frown, sad, andry or pouting', ':‑c': 'Frown, sad, andry or pouting', ':c': 'Frown, sad, andry or pouting', ':‑<': 'Frown, sad, andry or pouting', ':<': 'Frown, sad, andry or pouting', ':‑[': 'Frown, sad, andry or pouting', ':[': 'Frown, sad, andry or pouting', ':-||': 'Frown, sad, andry or pouting', '>:[': 'Frown, sad, andry or pouting', ':{': 'Frown, sad, andry or pouting', ':@': 'Frown, sad, andry or pouting', '>:(': 'Frown, sad, andry or pouting', \":'‑(\": 'Crying', \":'(\": 'Crying', \":'‑)\": 'Tears of happiness', \":')\": 'Tears of happiness', \"D‑':\": 'Horror', 'D:<': 'Disgust', 'D:': 'Sadness', 'D8': 'Great dismay', 'D;': 'Great dismay', 'D=': 'Great dismay', 'DX': 'Great dismay', ':‑O': 'Surprise', ':O': 'Surprise', ':‑o': 'Surprise', ':o': 'Surprise', ':-0': 'Shock', '8‑0': 'Yawn', '>:O': 'Yawn', ':-*': 'Kiss', ':*': 'Kiss', ':X': 'Kiss', ';‑)': 'Wink or smirk', ';)': 'Wink or smirk', '*-)': 'Wink or smirk', '*)': 'Wink or smirk', ';‑]': 'Wink or smirk', ';]': 'Wink or smirk', ';^)': 'Wink or smirk', ':‑,': 'Wink or smirk', ';D': 'Wink or smirk', ':‑P': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':P': 'Tongue sticking out, cheeky, playful or blowing a raspberry', 'X‑P': 'Tongue sticking out, cheeky, playful or blowing a raspberry', 'XP': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':‑Þ': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':Þ': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':b': 'Tongue sticking out, cheeky, playful or blowing a raspberry', 'd:': 'Tongue sticking out, cheeky, playful or blowing a raspberry', '=p': 'Tongue sticking out, cheeky, playful or blowing a raspberry', '>:P': 'Tongue sticking out, cheeky, playful or blowing a raspberry', ':‑/': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':/': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':-[.]': 'Skeptical, annoyed, undecided, uneasy or hesitant', '>:[(\\\\)]': 'Skeptical, annoyed, undecided, uneasy or hesitant', '>:/': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':[(\\\\)]': 'Skeptical, annoyed, undecided, uneasy or hesitant', '=/': 'Skeptical, annoyed, undecided, uneasy or hesitant', '=[(\\\\)]': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':L': 'Skeptical, annoyed, undecided, uneasy or hesitant', '=L': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':S': 'Skeptical, annoyed, undecided, uneasy or hesitant', ':‑|': 'Straight face', ':|': 'Straight face', ':$': 'Embarrassed or blushing', ':‑x': 'Sealed lips or wearing braces or tongue-tied', ':x': 'Sealed lips or wearing braces or tongue-tied', ':‑#': 'Sealed lips or wearing braces or tongue-tied', ':#': 'Sealed lips or wearing braces or tongue-tied', ':‑&': 'Sealed lips or wearing braces or tongue-tied', ':&': 'Sealed lips or wearing braces or tongue-tied', 'O:‑)': 'Angel, saint or innocent', 'O:)': 'Angel, saint or innocent', '0:‑3': 'Angel, saint or innocent', '0:3': 'Angel, saint or innocent', '0:‑)': 'Angel, saint or innocent', '0:)': 'Angel, saint or innocent', ':‑b': 'Tongue sticking out, cheeky, playful or blowing a raspberry', '0;^)': 'Angel, saint or innocent', '>:‑)': 'Evil or devilish', '>:)': 'Evil or devilish', '}:‑)': 'Evil or devilish', '}:)': 'Evil or devilish', '3:‑)': 'Evil or devilish', '3:)': 'Evil or devilish', '>;)': 'Evil or devilish', '|;‑)': 'Cool', '|‑O': 'Bored', ':‑J': 'Tongue-in-cheek', '#‑)': 'Party all night', '%‑)': 'Drunk or confused', '%)': 'Drunk or confused', ':-###..': 'Being sick', ':###..': 'Being sick', '<:‑|': 'Dump', '(>_<)': 'Troubled', '(>_<)>': 'Troubled', \"(';')\": 'Baby', '(^^>``': 'Nervous or Embarrassed or Troubled or Shy or Sweat drop', '(^_^;)': 'Nervous or Embarrassed or Troubled or Shy or Sweat drop', '(-_-;)': 'Nervous or Embarrassed or Troubled or Shy or Sweat drop', '(~_~;) (・.・;)': 'Nervous or Embarrassed or Troubled or Shy or Sweat drop', '(-_-)zzz': 'Sleeping', '(^_-)': 'Wink', '((+_+))': 'Confused', '(+o+)': 'Confused', '(o|o)': 'Ultraman', '^_^': 'Joyful', '(^_^)/': 'Joyful', '(^O^)／': 'Joyful', '(^o^)／': 'Joyful', '(__)': 'Kowtow as a sign of respect, or dogeza for apology', '_(._.)_': 'Kowtow as a sign of respect, or dogeza for apology', '<(_ _)>': 'Kowtow as a sign of respect, or dogeza for apology', '<m(__)m>': 'Kowtow as a sign of respect, or dogeza for apology', 'm(__)m': 'Kowtow as a sign of respect, or dogeza for apology', 'm(_ _)m': 'Kowtow as a sign of respect, or dogeza for apology', \"('_')\": 'Sad or Crying', '(/_;)': 'Sad or Crying', '(T_T) (;_;)': 'Sad or Crying', '(;_;': 'Sad of Crying', '(;_:)': 'Sad or Crying', '(;O;)': 'Sad or Crying', '(:_;)': 'Sad or Crying', '(ToT)': 'Sad or Crying', ';_;': 'Sad or Crying', ';-;': 'Sad or Crying', ';n;': 'Sad or Crying', ';;': 'Sad or Crying', 'Q.Q': 'Sad or Crying', 'T.T': 'Sad or Crying', 'QQ': 'Sad or Crying', 'Q_Q': 'Sad or Crying', '(-.-)': 'Shame', '(-_-)': 'Shame', '(一一)': 'Shame', '(；一_一)': 'Shame', '(=_=)': 'Tired', '(=^·^=)': 'cat', '(=^··^=)': 'cat', '=_^= ': 'cat', '(..)': 'Looking down', '(._.)': 'Looking down', '^m^': 'Giggling with hand covering mouth', '(・・?': 'Confusion', '(?_?)': 'Confusion', '>^_^<': 'Normal Laugh', '<^!^>': 'Normal Laugh', '^/^': 'Normal Laugh', '（*^_^*）': 'Normal Laugh', '(^<^) (^.^)': 'Normal Laugh', '(^^)': 'Normal Laugh', '(^.^)': 'Normal Laugh', '(^_^.)': 'Normal Laugh', '(^_^)': 'Normal Laugh', '(^J^)': 'Normal Laugh', '(*^.^*)': 'Normal Laugh', '(^—^）': 'Normal Laugh', '(#^.^#)': 'Normal Laugh', '（^—^）': 'Waving', '(;_;)/~~~': 'Waving', '(^.^)/~~~': 'Waving', '(-_-)/~~~ ($··)/~~~': 'Waving', '(T_T)/~~~': 'Waving', '(ToT)/~~~': 'Waving', '(*^0^*)': 'Excited', '(*_*)': 'Amazed', '(*_*;': 'Amazed', '(+_+) (@_@)': 'Amazed', '(*^^)v': 'Laughing,Cheerful', '(^_^)v': 'Laughing,Cheerful', '((d[-_-]b))': 'Headphones,Listening to music', '(-\"-)': 'Worried', '(ーー;)': 'Worried', '(^0_0^)': 'Eyeglasses', '(＾ｖ＾)': 'Happy', '(＾ｕ＾)': 'Happy', '(^)o(^)': 'Happy', '(^O^)': 'Happy', '(^o^)': 'Happy', ')^o^(': 'Happy', ':O o_O': 'Surprised', 'o_0': 'Surprised', 'o.O': 'Surpised', '(o.o)': 'Surprised', 'oO': 'Surprised', '(*￣m￣)': 'Dissatisfied', '(‘A`)': 'Snubbed or Deflated'}\n"
     ]
    }
   ],
   "source": [
    "print(EMOTICONS_EMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:20:00</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "5          0  1467811372         joy_wolf   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "5                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0    - Awww, that's a bummer.  You shoulda got Da... 2009-04-06 22:19:45   \n",
       "1  is upset that he can't update his Facebook by ... 2009-04-06 22:19:49   \n",
       "2   I dived many times for the ball. Managed to s... 2009-04-06 22:19:53   \n",
       "3    my whole body feels itchy and like its on fire  2009-04-06 22:19:57   \n",
       "4   no, it's not behaving at all. i'm mad. why am... 2009-04-06 22:19:57   \n",
       "5                                not the whole crew  2009-04-06 22:20:00   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "5  2009      4        0  22:20:00  ...       20-24          5            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "5                 0              0          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "5        0  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handling Emojis\n",
    "# Function for converting emojis into word\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = text.replace(emot, \"\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Apply Formula to tweets\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: convert_emoticons(x))\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ollin\\AppData\\Local\\Temp\\ipykernel_16552\\3552357475.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['tweet_tokenized'] = data['tweet_tokenized'].str.replace('[^\\w\\s]', '')\n",
      "C:\\Users\\ollin\\AppData\\Local\\Temp\\ipykernel_16552\\3552357475.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['tweet_tokenized'] = data['tweet_tokenized'].str.replace('\\d+', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww thats a bummer  you shoulda got david car...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0  awww thats a bummer  you shoulda got david car... 2009-04-06 22:19:45   \n",
       "1  is upset that he cant update his facebook by t... 2009-04-06 22:19:49   \n",
       "2  i dived many times for the ball managed to sav... 2009-04-06 22:19:53   \n",
       "3     my whole body feels itchy and like its on fire 2009-04-06 22:19:57   \n",
       "4  no its not behaving at all im mad why am i her... 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove all punctuation from the tweets\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].str.replace('[^\\w\\s]', '')\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].str.replace('_', '')\n",
    "\n",
    "#Remove all whitespaces from the beginning or end of the tweets\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].str.strip()\n",
    "\n",
    "#Set all characters to lowercase\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: x.lower())\n",
    "\n",
    "#Remove all numbers\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].str.replace('\\d+', '')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[awww, thats, a, bummer, you, shoulda, got, da...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0  [awww, thats, a, bummer, you, shoulda, got, da... 2009-04-06 22:19:45   \n",
       "1  [is, upset, that, he, cant, update, his, faceb... 2009-04-06 22:19:49   \n",
       "2  [i, dived, many, times, for, the, ball, manage... 2009-04-06 22:19:53   \n",
       "3  [my, whole, body, feels, itchy, and, like, its... 2009-04-06 22:19:57   \n",
       "4  [no, its, not, behaving, at, all, im, mad, why... 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize sentences based on non-alphanumeric characters (Leerstelle)\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: tokenize(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, rest...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0  [awww, thats, bummer, shoulda, got, david, car... 2009-04-06 22:19:45   \n",
       "1  [upset, cant, update, facebook, texting, might... 2009-04-06 22:19:49   \n",
       "2  [dived, many, times, ball, managed, save, rest... 2009-04-06 22:19:53   \n",
       "3            [whole, body, feels, itchy, like, fire] 2009-04-06 22:19:57   \n",
       "4                     [behaving, im, mad, cant, see] 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list): #Function to remove all stopword from our list of tokenized tweets\n",
    "    text = [word for word in tokenized_list if word not in stopword_clean] #Write each word from our tokenized list into a new list, if it is not in the stopword list\n",
    "    return text\n",
    "\n",
    "#Create new column with tokenized tweets without stopwords\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: remove_stopwords(x)) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dived, many, time, ball, managed, save, rest,...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0  [awww, thats, bummer, shoulda, got, david, car... 2009-04-06 22:19:45   \n",
       "1  [upset, cant, update, facebook, texting, might... 2009-04-06 22:19:49   \n",
       "2  [dived, many, time, ball, managed, save, rest,... 2009-04-06 22:19:53   \n",
       "3             [whole, body, feel, itchy, like, fire] 2009-04-06 22:19:57   \n",
       "4                     [behaving, im, mad, cant, see] 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "wnlemm = nltk.WordNetLemmatizer() \n",
    "\n",
    "def lemmatizing(tokenized_text): #Function to lemmatize all words in our tokenized tweets list without stopwords\n",
    "    text = [wnlemm.lemmatize(word) for word in tokenized_text] #Lemmatize each word in our tokenized list and write it into a new list\n",
    "    return text\n",
    "\n",
    "#Create new column with lemmatized tweets from our tokenized tweets without stopwords\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweet_tokenized_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dived, many, time, ball, managed, save, rest,...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dived many time ball managed save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>[thewdbcom, cool, hear, old, walt, interview, â]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thewdbcom cool hear old walt interview â</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>[ready, mojo, makeover, ask, detail]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>[happy, th, birthday, boo, alll, time, tupac, ...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happy th birthday boo alll time tupac amaru sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:50</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id             user  \\\n",
       "0                0  1467810369  _TheSpecialOne_   \n",
       "1                0  1467810672    scotthamilton   \n",
       "2                0  1467810917         mattycus   \n",
       "3                0  1467811184          ElleCTF   \n",
       "4                0  1467811193           Karoli   \n",
       "...            ...         ...              ...   \n",
       "1599995          1  2193601966  AmandaMarie1028   \n",
       "1599996          1  2193601969      TheWDBoards   \n",
       "1599997          1  2193601991           bpbabe   \n",
       "1599998          1  2193602064     tinydiamondz   \n",
       "1599999          1  2193602129   RyanTrevMorris   \n",
       "\n",
       "                                                     tweet  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1        is upset that he can't update his Facebook by ...   \n",
       "2        @Kenichan I dived many times for the ball. Man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1599995  Just woke up. Having no school is the best fee...   \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                           tweet_tokenized  \\\n",
       "0        [awww, thats, bummer, shoulda, got, david, car...   \n",
       "1        [upset, cant, update, facebook, texting, might...   \n",
       "2        [dived, many, time, ball, managed, save, rest,...   \n",
       "3                   [whole, body, feel, itchy, like, fire]   \n",
       "4                           [behaving, im, mad, cant, see]   \n",
       "...                                                    ...   \n",
       "1599995                [woke, school, best, feeling, ever]   \n",
       "1599996   [thewdbcom, cool, hear, old, walt, interview, â]   \n",
       "1599997               [ready, mojo, makeover, ask, detail]   \n",
       "1599998  [happy, th, birthday, boo, alll, time, tupac, ...   \n",
       "1599999                                            [happy]   \n",
       "\n",
       "                       date  year  month  weekday      time  ...  word_count  \\\n",
       "0       2009-04-06 22:19:45  2009      4        0  22:19:45  ...          19   \n",
       "1       2009-04-06 22:19:49  2009      4        0  22:19:49  ...          21   \n",
       "2       2009-04-06 22:19:53  2009      4        0  22:19:53  ...          18   \n",
       "3       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          10   \n",
       "4       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          21   \n",
       "...                     ...   ...    ...      ...       ...  ...         ...   \n",
       "1599995 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599996 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599997 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599998 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          12   \n",
       "1599999 2009-06-16 08:40:50  2009      6        1  08:40:50  ...           5   \n",
       "\n",
       "        dot_dot_dot  exclamation_mark  question_mark  at_symbol  link  money  \\\n",
       "0                 0                 0              0          1     1      0   \n",
       "1                 1                 1              0          0     0      0   \n",
       "2                 0                 0              0          1     0      0   \n",
       "3                 0                 0              0          0     0      0   \n",
       "4                 0                 0              1          1     0      0   \n",
       "...             ...               ...            ...        ...   ...    ...   \n",
       "1599995           0                 0              0          0     0      0   \n",
       "1599996           0                 1              0          0     1      0   \n",
       "1599997           0                 0              1          0     0      0   \n",
       "1599998           0                 1              0          0     0      0   \n",
       "1599999           0                 0              0          1     0      0   \n",
       "\n",
       "         paragraph_symbol  hashtag  \\\n",
       "0                       0        0   \n",
       "1                       0        0   \n",
       "2                       0        0   \n",
       "3                       0        0   \n",
       "4                       0        0   \n",
       "...                   ...      ...   \n",
       "1599995                 0        0   \n",
       "1599996                 0        0   \n",
       "1599997                 0        0   \n",
       "1599998                 0        0   \n",
       "1599999                 0        1   \n",
       "\n",
       "                                    tweet_tokenized_string  \n",
       "0        awww thats bummer shoulda got david carr third...  \n",
       "1        upset cant update facebook texting might cry r...  \n",
       "2          dived many time ball managed save rest go bound  \n",
       "3                          whole body feel itchy like fire  \n",
       "4                                 behaving im mad cant see  \n",
       "...                                                    ...  \n",
       "1599995                      woke school best feeling ever  \n",
       "1599996           thewdbcom cool hear old walt interview â  \n",
       "1599997                     ready mojo makeover ask detail  \n",
       "1599998  happy th birthday boo alll time tupac amaru sh...  \n",
       "1599999                                              happy  \n",
       "\n",
       "[1600000 rows x 22 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N-gram vectorizing and tfidf need a list of strings passed to it, so we need to convert our list\n",
    "data['tweet_tokenized_string'] = data['tweet_tokenized'].apply(lambda x: ' '.join(x)) #Join each word in our list with a space inbetween\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(data.tweet_tokenized_string.str.split(expand=True).stack().value_counts()).reset_index()\n",
    "words.rename(columns={'index': 'word', 0: 'count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51656</th>\n",
       "      <td>clientquot</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51657</th>\n",
       "      <td>nothinn</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51658</th>\n",
       "      <td>foulest</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51659</th>\n",
       "      <td>interception</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51660</th>\n",
       "      <td>morganampjudybffs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408938</th>\n",
       "      <td>jimena</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408939</th>\n",
       "      <td>funlicous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408940</th>\n",
       "      <td>beingohand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408941</th>\n",
       "      <td>ustreamive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408942</th>\n",
       "      <td>thewdbcom</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357287 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     word  count\n",
       "51656          clientquot      4\n",
       "51657             nothinn      4\n",
       "51658             foulest      4\n",
       "51659        interception      4\n",
       "51660   morganampjudybffs      4\n",
       "...                   ...    ...\n",
       "408938             jimena      1\n",
       "408939          funlicous      1\n",
       "408940         beingohand      1\n",
       "408941         ustreamive      1\n",
       "408942          thewdbcom      1\n",
       "\n",
       "[357287 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_split = words[words['count'] < 5]\n",
    "words_to_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clientquot',\n",
       " 'nothinn',\n",
       " 'foulest',\n",
       " 'interception',\n",
       " 'morganampjudybffs',\n",
       " 'cofffee',\n",
       " 'towny',\n",
       " 'dolces',\n",
       " 'mongoose',\n",
       " 'quotsomebodyquot',\n",
       " 'greaattt',\n",
       " 'seja',\n",
       " 'irregularly',\n",
       " 'sunni',\n",
       " 'pehle',\n",
       " 'anywa',\n",
       " 'awwwwwwwwwwwwwwwww',\n",
       " 'sarkozy',\n",
       " 'internetconnection',\n",
       " 'malapit',\n",
       " 'ilovejb',\n",
       " 'crossstitch',\n",
       " 'beastquot',\n",
       " 'prati',\n",
       " 'paquot',\n",
       " 'winkorsmirklt',\n",
       " 'incidentquot',\n",
       " 'maaa',\n",
       " 'immunisation',\n",
       " 'krugman',\n",
       " 'lolyeahi',\n",
       " 'borther',\n",
       " 'nowhad',\n",
       " 'quotadam',\n",
       " 'chiara',\n",
       " 'showtunes',\n",
       " 'ejaculate',\n",
       " 'leechers',\n",
       " 'kidsll',\n",
       " 'basia',\n",
       " 'assload',\n",
       " 'gratifying',\n",
       " 'quotpain',\n",
       " 'schoolcant',\n",
       " 'sett',\n",
       " 'arrey',\n",
       " 'babiesquot',\n",
       " 'topoptimizercom',\n",
       " 'rutland',\n",
       " 'lauging',\n",
       " 'palak',\n",
       " 'musle',\n",
       " 'decider',\n",
       " 'sorcery',\n",
       " 'hahathe',\n",
       " 'fabrice',\n",
       " 'solat',\n",
       " 'dyrdeks',\n",
       " 'saturdaysunday',\n",
       " 'updatetonguestickingoutcheekyplayfulorblowingaraspberry',\n",
       " 'witter',\n",
       " 'giggin',\n",
       " 'canx',\n",
       " 'airprt',\n",
       " 'warnin',\n",
       " 'dearquot',\n",
       " 'yayquot',\n",
       " 'takinq',\n",
       " 'petfinder',\n",
       " 'usso',\n",
       " 'soooory',\n",
       " 'sungod',\n",
       " 'sundaymonday',\n",
       " 'preformances',\n",
       " 'joyyy',\n",
       " 'uthe',\n",
       " 'hmmthat',\n",
       " 'reallt',\n",
       " 'incidently',\n",
       " 'emulation',\n",
       " 'againthanks',\n",
       " 'roberson',\n",
       " 'misinformed',\n",
       " 'ãããã',\n",
       " 'beijinhos',\n",
       " 'kasih',\n",
       " 'tumblrity',\n",
       " 'deanquot',\n",
       " 'yany',\n",
       " 'twestival',\n",
       " 'malmã',\n",
       " 'mysapce',\n",
       " 'iiiim',\n",
       " 'degs',\n",
       " 'chesterday',\n",
       " 'frontrow',\n",
       " 'forclosure',\n",
       " 'hodir',\n",
       " 'thoughill',\n",
       " 'shaye',\n",
       " 'claudio',\n",
       " 'bucking',\n",
       " 'twiiters',\n",
       " 'tomorrownext',\n",
       " 'grouchland',\n",
       " 'bundling',\n",
       " 'enchant',\n",
       " 'gilmour',\n",
       " 'quotjourney',\n",
       " 'fitna',\n",
       " 'heardread',\n",
       " 'exhib',\n",
       " 'pmo',\n",
       " 'willhave',\n",
       " 'penzance',\n",
       " 'idiom',\n",
       " 'leicestershire',\n",
       " 'plsql',\n",
       " 'yoh',\n",
       " 'pointsquot',\n",
       " 'dewy',\n",
       " 'gamewe',\n",
       " 'hayle',\n",
       " 'despicable',\n",
       " 'smartpunk',\n",
       " 'pice',\n",
       " 'hemlock',\n",
       " 'leslies',\n",
       " 'fasssst',\n",
       " 'actally',\n",
       " 'evading',\n",
       " 'crevã',\n",
       " 'chekhov',\n",
       " 'ndtv',\n",
       " 'eeeks',\n",
       " 'oooooohhh',\n",
       " 'dropd',\n",
       " 'aje',\n",
       " 'jrod',\n",
       " 'misser',\n",
       " 'valeries',\n",
       " 'unjailbroken',\n",
       " 'handwrist',\n",
       " 'hoarding',\n",
       " 'kpmg',\n",
       " 'reenergized',\n",
       " 'quotgonna',\n",
       " 'muchoo',\n",
       " 'mosley',\n",
       " 'telephoto',\n",
       " 'realityquot',\n",
       " 'bestfrann',\n",
       " 'phills',\n",
       " 'unactivated',\n",
       " 'baily',\n",
       " 'whewww',\n",
       " 'severity',\n",
       " 'urrghh',\n",
       " 'vondelpark',\n",
       " 'tweeeting',\n",
       " 'enjoyd',\n",
       " 'staing',\n",
       " 'neena',\n",
       " 'roomagain',\n",
       " 'morninn',\n",
       " 'focussing',\n",
       " 'happymy',\n",
       " 'southwark',\n",
       " 'sitc',\n",
       " 'ltwinkorsmirk',\n",
       " 'migth',\n",
       " 'engagment',\n",
       " 'barbecuing',\n",
       " 'prance',\n",
       " 'ahahahahahahaha',\n",
       " 'fanfuckintastic',\n",
       " 'veggin',\n",
       " 'gyaan',\n",
       " 'kake',\n",
       " 'quottestingquot',\n",
       " 'dopeee',\n",
       " 'adu',\n",
       " 'editorquot',\n",
       " 'girliee',\n",
       " 'spellbound',\n",
       " 'heyyyyyyyyyyyy',\n",
       " 'dkny',\n",
       " 'distillery',\n",
       " 'realllll',\n",
       " 'ððð¾ñ',\n",
       " 'farina',\n",
       " 'confcall',\n",
       " 'themselvesquot',\n",
       " 'callsquot',\n",
       " 'tanka',\n",
       " 'tonightthat',\n",
       " 'haaaha',\n",
       " 'gourgeous',\n",
       " 'cabby',\n",
       " 'bidet',\n",
       " 'ewen',\n",
       " 'offcant',\n",
       " 'errrm',\n",
       " 'dressd',\n",
       " 'quotchain',\n",
       " 'tonightenjoy',\n",
       " 'masonic',\n",
       " 'rightyo',\n",
       " 'matamoros',\n",
       " 'emimen',\n",
       " 'riah',\n",
       " 'cutts',\n",
       " 'oneshots',\n",
       " 'kyknya',\n",
       " 'gouge',\n",
       " 'mesomeone',\n",
       " 'errrrrrr',\n",
       " 'midwinter',\n",
       " 'eventsquot',\n",
       " 'dilute',\n",
       " 'wehn',\n",
       " 'pageviews',\n",
       " 'lolnight',\n",
       " 'twittwit',\n",
       " 'fromt',\n",
       " 'ehmm',\n",
       " 'irenes',\n",
       " 'dfc',\n",
       " 'jacque',\n",
       " 'fli',\n",
       " 'soldiering',\n",
       " 'disallowed',\n",
       " 'ffxii',\n",
       " 'moonday',\n",
       " 'missoula',\n",
       " 'dawanda',\n",
       " 'snls',\n",
       " 'bandand',\n",
       " 'ugggggg',\n",
       " 'bahahahahah',\n",
       " 'quotsisterquot',\n",
       " 'breakkie',\n",
       " 'mooni',\n",
       " 'aaall',\n",
       " 'pru',\n",
       " 'wedfri',\n",
       " 'procastination',\n",
       " 'dubliner',\n",
       " 'obtaining',\n",
       " 'portillos',\n",
       " 'liquorice',\n",
       " 'ezinearticlescom',\n",
       " 'mabes',\n",
       " 'wooohoooooo',\n",
       " 'roadbut',\n",
       " 'beckoning',\n",
       " 'sophmores',\n",
       " 'readwrite',\n",
       " 'excludes',\n",
       " 'glancing',\n",
       " 'marlie',\n",
       " 'powquot',\n",
       " 'zycam',\n",
       " 'anyquot',\n",
       " 'quotwinkorsmirkoes',\n",
       " 'erudite',\n",
       " 'quietquot',\n",
       " 'ttysoon',\n",
       " 'cic',\n",
       " 'shotquot',\n",
       " 'cannotquot',\n",
       " 'prohibits',\n",
       " 'workoff',\n",
       " 'khair',\n",
       " 'jeg',\n",
       " 'quotinsert',\n",
       " 'denomination',\n",
       " 'bientãt',\n",
       " 'lowlight',\n",
       " 'paradeeee',\n",
       " 'itoff',\n",
       " 'snuggies',\n",
       " 'dropoff',\n",
       " 'seasoni',\n",
       " 'creeeepy',\n",
       " 'tolstoy',\n",
       " 'sqaure',\n",
       " 'commenters',\n",
       " 'whoaoh',\n",
       " 'sonhe',\n",
       " 'amerika',\n",
       " 'xkristie',\n",
       " 'kuku',\n",
       " 'guadalupe',\n",
       " 'oneeeeeee',\n",
       " 'stjean',\n",
       " 'thinklol',\n",
       " 'flaunting',\n",
       " 'quotblahquot',\n",
       " 'elfen',\n",
       " 'competes',\n",
       " 'nickk',\n",
       " 'knowwwwwww',\n",
       " 'novio',\n",
       " 'misuse',\n",
       " 'widnes',\n",
       " 'inas',\n",
       " 'nerdcore',\n",
       " 'motherfcker',\n",
       " 'geoffs',\n",
       " 'uhhhm',\n",
       " 'anywy',\n",
       " 'informer',\n",
       " 'rhiannons',\n",
       " 'usng',\n",
       " 'schick',\n",
       " 'splattered',\n",
       " 'svz',\n",
       " 'doinggg',\n",
       " 'characterization',\n",
       " 'meghann',\n",
       " 'eardrop',\n",
       " 'tonightyou',\n",
       " 'taylorr',\n",
       " 'weatherrrr',\n",
       " 'eastridge',\n",
       " 'shalll',\n",
       " 'wohoooooo',\n",
       " 'lovvveee',\n",
       " 'unexcited',\n",
       " 'luãn',\n",
       " 'summerfeeling',\n",
       " 'exitedd',\n",
       " 'doux',\n",
       " 'feburary',\n",
       " 'twooo',\n",
       " 'dalek',\n",
       " 'uuuuuugh',\n",
       " 'jerm',\n",
       " 'bagger',\n",
       " 'surveying',\n",
       " 'loveslt',\n",
       " 'wonderwoman',\n",
       " 'platformer',\n",
       " 'harri',\n",
       " 'noooooow',\n",
       " 'raffa',\n",
       " 'chatquot',\n",
       " 'windown',\n",
       " 'muaaah',\n",
       " 'yammi',\n",
       " 'svenja',\n",
       " 'kelsi',\n",
       " 'downjust',\n",
       " 'gravitational',\n",
       " 'schlitterbahn',\n",
       " 'blaisdell',\n",
       " 'altoids',\n",
       " 'atf',\n",
       " 'dlna',\n",
       " 'vika',\n",
       " 'subuh',\n",
       " 'meampmy',\n",
       " 'todaythats',\n",
       " 'loststolen',\n",
       " 'cornet',\n",
       " 'waaaaayyyyy',\n",
       " 'ðññð',\n",
       " 'youtubeare',\n",
       " 'bejesus',\n",
       " 'wilted',\n",
       " 'gyp',\n",
       " 'chama',\n",
       " 'escala',\n",
       " 'fairrrrrr',\n",
       " 'fieldwork',\n",
       " 'bummmer',\n",
       " 'taqueria',\n",
       " 'telephony',\n",
       " 'picutre',\n",
       " 'thankie',\n",
       " 'cptn',\n",
       " 'doppler',\n",
       " 'myv',\n",
       " 'conte',\n",
       " 'greeeaaat',\n",
       " 'examined',\n",
       " 'twalk',\n",
       " 'noothing',\n",
       " 'slapstick',\n",
       " 'braws',\n",
       " 'zonk',\n",
       " 'snowdonia',\n",
       " 'nilai',\n",
       " 'cbe',\n",
       " 'jonjon',\n",
       " 'chn',\n",
       " 'aqa',\n",
       " 'wellonly',\n",
       " 'pizzaaaa',\n",
       " 'scriptquot',\n",
       " 'frgt',\n",
       " 'codie',\n",
       " 'hmmmwhy',\n",
       " 'weekendtoo',\n",
       " 'hahahhahaa',\n",
       " 'goodlookin',\n",
       " 'tailwind',\n",
       " 'airporti',\n",
       " 'wednesay',\n",
       " 'loooooads',\n",
       " 'fatherson',\n",
       " 'entendres',\n",
       " 'lsc',\n",
       " 'heyits',\n",
       " 'comunication',\n",
       " 'razorscale',\n",
       " 'myspacefacebook',\n",
       " 'spongbob',\n",
       " 'greentea',\n",
       " 'prewedding',\n",
       " 'amout',\n",
       " 'twittertown',\n",
       " 'quotstrongerquot',\n",
       " 'actquot',\n",
       " 'fairing',\n",
       " 'stuffz',\n",
       " 'againwill',\n",
       " 'haka',\n",
       " 'ughhhmy',\n",
       " 'defqon',\n",
       " 'flipin',\n",
       " 'salud',\n",
       " 'themeforest',\n",
       " 'heyyaa',\n",
       " 'toz',\n",
       " 'outhe',\n",
       " 'illa',\n",
       " 'tiana',\n",
       " 'myspacein',\n",
       " 'nipper',\n",
       " 'issacs',\n",
       " 'overground',\n",
       " 'liiiike',\n",
       " 'meatquot',\n",
       " 'carlsberg',\n",
       " 'owey',\n",
       " 'onlineit',\n",
       " 'toniiiight',\n",
       " 'missk',\n",
       " 'deena',\n",
       " 'nyonya',\n",
       " 'unusualsquot',\n",
       " 'breakits',\n",
       " 'fuhn',\n",
       " 'hmmmmi',\n",
       " 'bodo',\n",
       " 'minder',\n",
       " 'tablespoon',\n",
       " 'kik',\n",
       " 'stately',\n",
       " 'evoo',\n",
       " 'shir',\n",
       " 'footsies',\n",
       " 'sleepwatching',\n",
       " 'mnms',\n",
       " 'achu',\n",
       " 'homa',\n",
       " 'phãt',\n",
       " 'skepticism',\n",
       " 'bcnwe',\n",
       " 'wurn',\n",
       " 'phlog',\n",
       " 'ipaq',\n",
       " 'cã³mo',\n",
       " 'vernacular',\n",
       " 'hitcher',\n",
       " 'bãªn',\n",
       " 'fixation',\n",
       " 'snorin',\n",
       " 'substantially',\n",
       " 'youngq',\n",
       " 'discipleship',\n",
       " 'indent',\n",
       " 'raddison',\n",
       " 'shuan',\n",
       " 'dumpy',\n",
       " 'minutebut',\n",
       " 'äæáng',\n",
       " 'thoughright',\n",
       " 'tmrrow',\n",
       " 'drivable',\n",
       " 'minitrip',\n",
       " 'comeeeee',\n",
       " 'mileymandyfan',\n",
       " 'cholera',\n",
       " 'staved',\n",
       " 'nkotbs',\n",
       " 'neutrogena',\n",
       " 'lanetea',\n",
       " 'wso',\n",
       " 'geta',\n",
       " 'begone',\n",
       " 'macromedia',\n",
       " 'lãºc',\n",
       " 'wdc',\n",
       " 'piace',\n",
       " 'frasers',\n",
       " 'nunggu',\n",
       " 'starvingggg',\n",
       " 'foodstuff',\n",
       " 'hahajust',\n",
       " 'edinburg',\n",
       " 'afternoonim',\n",
       " 'bigups',\n",
       " 'posy',\n",
       " 'siriusxm',\n",
       " 'stacys',\n",
       " 'cockblocked',\n",
       " 'apocalyptic',\n",
       " 'frisk',\n",
       " 'fick',\n",
       " 'minolta',\n",
       " 'yayyyyyyyyyyy',\n",
       " 'ches',\n",
       " 'eugenics',\n",
       " 'togehter',\n",
       " 'phily',\n",
       " 'driveee',\n",
       " 'londoni',\n",
       " 'thaan',\n",
       " 'superpreview',\n",
       " 'bampbs',\n",
       " 'slh',\n",
       " 'herewish',\n",
       " 'torchys',\n",
       " 'citadel',\n",
       " 'deadend',\n",
       " 'occurence',\n",
       " 'skarsgard',\n",
       " 'aryt',\n",
       " 'gusty',\n",
       " 'alk',\n",
       " 'soomuch',\n",
       " 'kvm',\n",
       " 'merv',\n",
       " 'didquot',\n",
       " 'atli',\n",
       " 'shirtwoot',\n",
       " 'joyride',\n",
       " 'oratory',\n",
       " 'piãa',\n",
       " 'shoppes',\n",
       " 'unpleasent',\n",
       " 'yooouu',\n",
       " 'govs',\n",
       " 'entah',\n",
       " 'rallying',\n",
       " 'reeli',\n",
       " 'finsbury',\n",
       " 'howwhere',\n",
       " 'shamus',\n",
       " 'putney',\n",
       " 'wondrful',\n",
       " 'mandated',\n",
       " 'caucasian',\n",
       " 'expertsquot',\n",
       " 'dafont',\n",
       " 'zhivago',\n",
       " 'pretttyyyyyyyyyyyyy',\n",
       " 'quotpink',\n",
       " 'natalieeee',\n",
       " 'jurrasic',\n",
       " 'imedia',\n",
       " 'gmailcom',\n",
       " 'coriandr',\n",
       " 'bupa',\n",
       " 'republish',\n",
       " 'sils',\n",
       " 'anywaybut',\n",
       " 'ralf',\n",
       " 'arountonguestickingoutcheekyplayfulorblowingaraspberry',\n",
       " 'ðððµ',\n",
       " 'pillion',\n",
       " 'whan',\n",
       " 'vibequot',\n",
       " 'angioplasty',\n",
       " 'comofas',\n",
       " 'disarray',\n",
       " 'lawrie',\n",
       " 'hok',\n",
       " 'roxana',\n",
       " 'moreeeeee',\n",
       " 'okit',\n",
       " 'jussi',\n",
       " 'awesomethanks',\n",
       " 'trytomakemoneyonlinecom',\n",
       " 'followeri',\n",
       " 'nester',\n",
       " 'churbs',\n",
       " 'fwb',\n",
       " 'retrieval',\n",
       " 'xbmc',\n",
       " 'plod',\n",
       " 'dnote',\n",
       " 'darnnit',\n",
       " 'uhs',\n",
       " 'tonightwho',\n",
       " 'thnking',\n",
       " 'loserish',\n",
       " 'andno',\n",
       " 'expirience',\n",
       " 'hotelquot',\n",
       " 'sero',\n",
       " 'scrapbooked',\n",
       " 'wiating',\n",
       " 'omh',\n",
       " 'folksi',\n",
       " 'croker',\n",
       " 'supras',\n",
       " 'happyfaceorsmileymoving',\n",
       " 'tbag',\n",
       " 'weightwatchers',\n",
       " 'hallucinate',\n",
       " 'hairsadorcrying',\n",
       " 'peux',\n",
       " 'pinaka',\n",
       " 'splotchy',\n",
       " 'nippon',\n",
       " 'headacheit',\n",
       " 'saguaro',\n",
       " 'congee',\n",
       " 'awesomenessquot',\n",
       " 'kettlecorn',\n",
       " 'turqoise',\n",
       " 'decapitation',\n",
       " 'quotloving',\n",
       " 'whoz',\n",
       " 'gumps',\n",
       " 'booskie',\n",
       " 'saturdaybut',\n",
       " 'quotmamma',\n",
       " 'emjay',\n",
       " 'wolfing',\n",
       " 'partit',\n",
       " 'biscayne',\n",
       " 'moda',\n",
       " 'mobster',\n",
       " 'disembodied',\n",
       " 'belives',\n",
       " 'jokebut',\n",
       " 'livo',\n",
       " 'golfball',\n",
       " 'macking',\n",
       " 'swizzle',\n",
       " 'gozaimasu',\n",
       " 'magazin',\n",
       " 'twilighti',\n",
       " 'eyewitness',\n",
       " 'sighhhhhh',\n",
       " 'twitterly',\n",
       " 'retracted',\n",
       " 'relaxi',\n",
       " 'favvv',\n",
       " 'wahhhhhhhh',\n",
       " 'barby',\n",
       " 'plsi',\n",
       " 'inversion',\n",
       " 'incerc',\n",
       " 'renewable',\n",
       " 'neruda',\n",
       " 'tralier',\n",
       " 'chikka',\n",
       " 'yeer',\n",
       " 'coveted',\n",
       " 'adgi',\n",
       " 'awesoem',\n",
       " 'manicotti',\n",
       " 'antrim',\n",
       " 'culminatings',\n",
       " 'happyfacesmileyhappyfacesmileyam',\n",
       " 'itï½ll',\n",
       " 'quotpet',\n",
       " 'liftime',\n",
       " 'freezequot',\n",
       " 'sshhh',\n",
       " 'chappell',\n",
       " 'heartz',\n",
       " 'hoomee',\n",
       " 'ruiz',\n",
       " 'deserting',\n",
       " 'ribeyes',\n",
       " 'ratatat',\n",
       " 'jussst',\n",
       " 'musicwant',\n",
       " 'gonzales',\n",
       " 'ilford',\n",
       " 'chroniclesquot',\n",
       " 'delts',\n",
       " 'voxcom',\n",
       " 'nowwhen',\n",
       " 'numpties',\n",
       " 'tetchy',\n",
       " 'ridee',\n",
       " 'mainframe',\n",
       " 'percussionist',\n",
       " 'morten',\n",
       " 'appï½tit',\n",
       " 'piet',\n",
       " 'gaffer',\n",
       " 'âu',\n",
       " 'counrty',\n",
       " 'geet',\n",
       " 'advisable',\n",
       " 'multifunctional',\n",
       " 'sundayz',\n",
       " 'yush',\n",
       " 'coolits',\n",
       " 'childline',\n",
       " 'kildare',\n",
       " 'albumn',\n",
       " 'intp',\n",
       " 'unexplainably',\n",
       " 'mork',\n",
       " 'forfun',\n",
       " 'complicating',\n",
       " 'fuunnn',\n",
       " 'twothree',\n",
       " 'zeit',\n",
       " 'netno',\n",
       " 'ideaaa',\n",
       " 'iphoneginger',\n",
       " 'ruim',\n",
       " 'quotvia',\n",
       " 'houswives',\n",
       " 'cocok',\n",
       " 'kicky',\n",
       " 'hmmyou',\n",
       " 'gurgle',\n",
       " 'mssed',\n",
       " 'tigh',\n",
       " 'concierto',\n",
       " 'amaziiiing',\n",
       " 'quotemail',\n",
       " 'trickn',\n",
       " 'monthsand',\n",
       " 'upfronts',\n",
       " 'jessss',\n",
       " 'bbylt',\n",
       " 'yodeling',\n",
       " 'dependant',\n",
       " 'twitering',\n",
       " 'sul',\n",
       " 'cpanel',\n",
       " 'rejuvinated',\n",
       " 'housegetting',\n",
       " 'yoube',\n",
       " 'supergirl',\n",
       " 'cankles',\n",
       " 'bantam',\n",
       " 'manthe',\n",
       " 'woring',\n",
       " 'woooooooot',\n",
       " 'pruned',\n",
       " 'storybook',\n",
       " 'tkin',\n",
       " 'wakeywakey',\n",
       " 'beachhhhh',\n",
       " 'repentance',\n",
       " 'marrakech',\n",
       " 'wooooooooooooooo',\n",
       " 'matcha',\n",
       " 'poooop',\n",
       " 'swiping',\n",
       " 'aaaaaaaaaaaaaaah',\n",
       " 'canr',\n",
       " 'instantlyquot',\n",
       " 'necktie',\n",
       " 'ramadan',\n",
       " 'faustï½o',\n",
       " 'yeeessss',\n",
       " 'embryo',\n",
       " 'precision',\n",
       " 'nadda',\n",
       " 'pooling',\n",
       " 'kayne',\n",
       " 'disparity',\n",
       " 'wussy',\n",
       " 'antiflag',\n",
       " 'complexity',\n",
       " 'delonte',\n",
       " 'thirtieth',\n",
       " 'heterosexual',\n",
       " 'hais',\n",
       " 'okaymaybe',\n",
       " 'tmp',\n",
       " 'gyukaku',\n",
       " 'hussain',\n",
       " 'blackheath',\n",
       " 'quotslowquot',\n",
       " 'tdai',\n",
       " 'quotsunday',\n",
       " 'feelingz',\n",
       " 'startim',\n",
       " 'framerate',\n",
       " 'chelan',\n",
       " 'kanta',\n",
       " 'dayyyys',\n",
       " 'wring',\n",
       " 'moderated',\n",
       " 'tinypic',\n",
       " 'quotms',\n",
       " 'wirelessly',\n",
       " 'stumptown',\n",
       " 'invovled',\n",
       " 'dammnit',\n",
       " 'morninggotta',\n",
       " 'rego',\n",
       " 'baster',\n",
       " 'dayuntil',\n",
       " 'ltnot',\n",
       " 'schlep',\n",
       " 'tendre',\n",
       " 'itched',\n",
       " 'juanita',\n",
       " 'morningwhat',\n",
       " 'whitty',\n",
       " 'todayx',\n",
       " 'iliana',\n",
       " 'speel',\n",
       " 'mondayhopefully',\n",
       " 'chipin',\n",
       " 'synagogue',\n",
       " 'bumalik',\n",
       " 'saran',\n",
       " 'firstt',\n",
       " 'gaybut',\n",
       " 'mitosis',\n",
       " 'sorrywas',\n",
       " 'accc',\n",
       " 'ðñðð¼ðµñ',\n",
       " 'scribd',\n",
       " 'brydon',\n",
       " 'outskirt',\n",
       " 'undeveloped',\n",
       " 'raa',\n",
       " 'vampy',\n",
       " 'whers',\n",
       " 'junoir',\n",
       " 'quoteverythingquot',\n",
       " 'bozeman',\n",
       " 'showthe',\n",
       " 'lessening',\n",
       " 'followe',\n",
       " 'cur',\n",
       " 'dramaa',\n",
       " 'cubed',\n",
       " 'escapism',\n",
       " 'cq',\n",
       " 'vergeten',\n",
       " 'jjb',\n",
       " 'rainbowquot',\n",
       " 'shtno',\n",
       " 'diedddd',\n",
       " 'soreand',\n",
       " 'longed',\n",
       " 'øøøø',\n",
       " 'quotwest',\n",
       " 'quotsource',\n",
       " 'loike',\n",
       " 'fertility',\n",
       " 'qud',\n",
       " 'choosed',\n",
       " 'gaf',\n",
       " 'omgwtf',\n",
       " 'elmwood',\n",
       " 'cuzi',\n",
       " 'manthis',\n",
       " 'yummmmmmmm',\n",
       " 'binh',\n",
       " 'ashington',\n",
       " 'extern',\n",
       " 'ð¼ðµð½ññðµ',\n",
       " 'jazmin',\n",
       " 'wookiees',\n",
       " 'tweetable',\n",
       " 'musos',\n",
       " 'decongestant',\n",
       " 'coldplaying',\n",
       " 'doodled',\n",
       " 'gareths',\n",
       " 'huggggs',\n",
       " 'boyfwend',\n",
       " 'keven',\n",
       " 'imyy',\n",
       " 'brownsville',\n",
       " 'nette',\n",
       " 'demure',\n",
       " 'crabcakes',\n",
       " 'oneoh',\n",
       " 'schizophrenia',\n",
       " 'fisty',\n",
       " 'nukka',\n",
       " 'daydamn',\n",
       " 'bejamin',\n",
       " 'fergies',\n",
       " 'mezzah',\n",
       " 'twipic',\n",
       " 'dav',\n",
       " 'zombieville',\n",
       " 'lightskin',\n",
       " 'bladerunner',\n",
       " 'ltrandomtextgt',\n",
       " 'superdry',\n",
       " 'dottie',\n",
       " 'yeats',\n",
       " 'postholiday',\n",
       " 'gede',\n",
       " 'mille',\n",
       " 'hddvd',\n",
       " 'dwele',\n",
       " 'omars',\n",
       " 'hairand',\n",
       " 'ampc',\n",
       " 'packingg',\n",
       " 'ntonguestickingoutcheekyplayfulorblowingaraspberry',\n",
       " 'gaucho',\n",
       " 'girlfrend',\n",
       " 'balmer',\n",
       " 'eithermy',\n",
       " 'disrupt',\n",
       " 'southafrica',\n",
       " 'mencari',\n",
       " 'setiap',\n",
       " 'shoppiin',\n",
       " 'hequots',\n",
       " 'lifecant',\n",
       " 'muon',\n",
       " 'apni',\n",
       " 'beardquot',\n",
       " 'floridai',\n",
       " 'propos',\n",
       " 'pufferfish',\n",
       " 'beeeest',\n",
       " 'tatort',\n",
       " 'wheatgrass',\n",
       " 'frolick',\n",
       " 'donelol',\n",
       " 'hijinx',\n",
       " 'moer',\n",
       " 'tipsquot',\n",
       " 'cheaps',\n",
       " 'whewwww',\n",
       " 'freeeeeeezing',\n",
       " 'aata',\n",
       " 'typer',\n",
       " 'thoughhope',\n",
       " 'millionair',\n",
       " 'gadge',\n",
       " 'tothom',\n",
       " 'newlook',\n",
       " 'andhra',\n",
       " 'tecates',\n",
       " 'holdquot',\n",
       " 'ptit',\n",
       " 'clothesand',\n",
       " 'prblm',\n",
       " 'lolwinkorsmirk',\n",
       " 'afforded',\n",
       " 'sueno',\n",
       " 'deteriorate',\n",
       " 'sleeplessness',\n",
       " 'taylorrrr',\n",
       " 'comeãando',\n",
       " 'babyish',\n",
       " 'ughhhi',\n",
       " 'haaaave',\n",
       " 'äáu',\n",
       " 'kristel',\n",
       " 'zlatan',\n",
       " 'wetland',\n",
       " 'kestrel',\n",
       " 'bestlt',\n",
       " 'quotwhoquot',\n",
       " 'quotclick',\n",
       " 'steed',\n",
       " 'stopi',\n",
       " 'shnap',\n",
       " 'nailing',\n",
       " 'knightley',\n",
       " 'rafaels',\n",
       " 'rolex',\n",
       " 'candyman',\n",
       " 'redrock',\n",
       " 'disadvantaged',\n",
       " 'milkman',\n",
       " 'tweetsim',\n",
       " 'jpgs',\n",
       " 'dicided',\n",
       " 'nemenin',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_split_list = words_to_split['word'].tolist()\n",
    "words_to_split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_use_words = set(words_to_split_list) # Convert your list to a set for faster lookup\n",
    "\n",
    "def remove_single_use_words(tweet):\n",
    "    return [word for word in tweet if word not in single_use_words]\n",
    "\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(remove_single_use_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweet_tokenized_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[many, time, ball, managed, save, rest, go, bo...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>many time ball managed save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>[cool, hear, old, walt, interview, â]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cool hear old walt interview â</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>[ready, mojo, makeover, ask, detail]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>[happy, th, birthday, boo, alll, time, tupac]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happy th birthday boo alll time tupac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:50</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id             user  \\\n",
       "0                0  1467810369  _TheSpecialOne_   \n",
       "1                0  1467810672    scotthamilton   \n",
       "2                0  1467810917         mattycus   \n",
       "3                0  1467811184          ElleCTF   \n",
       "4                0  1467811193           Karoli   \n",
       "...            ...         ...              ...   \n",
       "1599995          1  2193601966  AmandaMarie1028   \n",
       "1599996          1  2193601969      TheWDBoards   \n",
       "1599997          1  2193601991           bpbabe   \n",
       "1599998          1  2193602064     tinydiamondz   \n",
       "1599999          1  2193602129   RyanTrevMorris   \n",
       "\n",
       "                                                     tweet  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1        is upset that he can't update his Facebook by ...   \n",
       "2        @Kenichan I dived many times for the ball. Man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1599995  Just woke up. Having no school is the best fee...   \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                           tweet_tokenized  \\\n",
       "0        [awww, thats, bummer, shoulda, got, david, car...   \n",
       "1        [upset, cant, update, facebook, texting, might...   \n",
       "2        [many, time, ball, managed, save, rest, go, bo...   \n",
       "3                   [whole, body, feel, itchy, like, fire]   \n",
       "4                           [behaving, im, mad, cant, see]   \n",
       "...                                                    ...   \n",
       "1599995                [woke, school, best, feeling, ever]   \n",
       "1599996              [cool, hear, old, walt, interview, â]   \n",
       "1599997               [ready, mojo, makeover, ask, detail]   \n",
       "1599998      [happy, th, birthday, boo, alll, time, tupac]   \n",
       "1599999                                            [happy]   \n",
       "\n",
       "                       date  year  month  weekday      time  ...  word_count  \\\n",
       "0       2009-04-06 22:19:45  2009      4        0  22:19:45  ...          19   \n",
       "1       2009-04-06 22:19:49  2009      4        0  22:19:49  ...          21   \n",
       "2       2009-04-06 22:19:53  2009      4        0  22:19:53  ...          18   \n",
       "3       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          10   \n",
       "4       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          21   \n",
       "...                     ...   ...    ...      ...       ...  ...         ...   \n",
       "1599995 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599996 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599997 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599998 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          12   \n",
       "1599999 2009-06-16 08:40:50  2009      6        1  08:40:50  ...           5   \n",
       "\n",
       "        dot_dot_dot  exclamation_mark  question_mark  at_symbol  link  money  \\\n",
       "0                 0                 0              0          1     1      0   \n",
       "1                 1                 1              0          0     0      0   \n",
       "2                 0                 0              0          1     0      0   \n",
       "3                 0                 0              0          0     0      0   \n",
       "4                 0                 0              1          1     0      0   \n",
       "...             ...               ...            ...        ...   ...    ...   \n",
       "1599995           0                 0              0          0     0      0   \n",
       "1599996           0                 1              0          0     1      0   \n",
       "1599997           0                 0              1          0     0      0   \n",
       "1599998           0                 1              0          0     0      0   \n",
       "1599999           0                 0              0          1     0      0   \n",
       "\n",
       "         paragraph_symbol  hashtag  \\\n",
       "0                       0        0   \n",
       "1                       0        0   \n",
       "2                       0        0   \n",
       "3                       0        0   \n",
       "4                       0        0   \n",
       "...                   ...      ...   \n",
       "1599995                 0        0   \n",
       "1599996                 0        0   \n",
       "1599997                 0        0   \n",
       "1599998                 0        0   \n",
       "1599999                 0        1   \n",
       "\n",
       "                                    tweet_tokenized_string  \n",
       "0        awww thats bummer shoulda got david carr third...  \n",
       "1        upset cant update facebook texting might cry r...  \n",
       "2                many time ball managed save rest go bound  \n",
       "3                          whole body feel itchy like fire  \n",
       "4                                 behaving im mad cant see  \n",
       "...                                                    ...  \n",
       "1599995                      woke school best feeling ever  \n",
       "1599996                     cool hear old walt interview â  \n",
       "1599997                     ready mojo makeover ask detail  \n",
       "1599998              happy th birthday boo alll time tupac  \n",
       "1599999                                              happy  \n",
       "\n",
       "[1600000 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N-gram vectorizing and tfidf need a list of strings passed to it, so we need to convert our list\n",
    "data['tweet_tokenized_string'] = data['tweet_tokenized'].apply(lambda x: ' '.join(x)) #Join each word in our list with a space inbetween\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wenn wir wollen, können wir diese Tweets alle herausfiltern (Dafür einfach das False in True umwandeln)\n",
    "data = data[data['tweet_tokenized_string'].str.contains('[a-zA-Z]')==True] # Alle Tweet herausfiltern die nur nicht normale Buchstaben enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweet_tokenized_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[many, time, ball, managed, save, rest, go, bo...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>many time ball managed save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587561</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587562</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>[cool, hear, old, walt, interview, â]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cool hear old walt interview â</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587563</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>[ready, mojo, makeover, ask, detail]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587564</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>[happy, th, birthday, boo, alll, time, tupac]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happy th birthday boo alll time tupac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587565</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:50</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1587566 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id             user  \\\n",
       "0                0  1467810369  _TheSpecialOne_   \n",
       "1                0  1467810672    scotthamilton   \n",
       "2                0  1467810917         mattycus   \n",
       "3                0  1467811184          ElleCTF   \n",
       "4                0  1467811193           Karoli   \n",
       "...            ...         ...              ...   \n",
       "1587561          1  2193601966  AmandaMarie1028   \n",
       "1587562          1  2193601969      TheWDBoards   \n",
       "1587563          1  2193601991           bpbabe   \n",
       "1587564          1  2193602064     tinydiamondz   \n",
       "1587565          1  2193602129   RyanTrevMorris   \n",
       "\n",
       "                                                     tweet  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1        is upset that he can't update his Facebook by ...   \n",
       "2        @Kenichan I dived many times for the ball. Man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1587561  Just woke up. Having no school is the best fee...   \n",
       "1587562  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1587563  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1587564  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1587565  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                           tweet_tokenized  \\\n",
       "0        [awww, thats, bummer, shoulda, got, david, car...   \n",
       "1        [upset, cant, update, facebook, texting, might...   \n",
       "2        [many, time, ball, managed, save, rest, go, bo...   \n",
       "3                   [whole, body, feel, itchy, like, fire]   \n",
       "4                           [behaving, im, mad, cant, see]   \n",
       "...                                                    ...   \n",
       "1587561                [woke, school, best, feeling, ever]   \n",
       "1587562              [cool, hear, old, walt, interview, â]   \n",
       "1587563               [ready, mojo, makeover, ask, detail]   \n",
       "1587564      [happy, th, birthday, boo, alll, time, tupac]   \n",
       "1587565                                            [happy]   \n",
       "\n",
       "                       date  year  month  weekday      time  ...  word_count  \\\n",
       "0       2009-04-06 22:19:45  2009      4        0  22:19:45  ...          19   \n",
       "1       2009-04-06 22:19:49  2009      4        0  22:19:49  ...          21   \n",
       "2       2009-04-06 22:19:53  2009      4        0  22:19:53  ...          18   \n",
       "3       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          10   \n",
       "4       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          21   \n",
       "...                     ...   ...    ...      ...       ...  ...         ...   \n",
       "1587561 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1587562 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1587563 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1587564 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          12   \n",
       "1587565 2009-06-16 08:40:50  2009      6        1  08:40:50  ...           5   \n",
       "\n",
       "        dot_dot_dot  exclamation_mark  question_mark  at_symbol  link  money  \\\n",
       "0                 0                 0              0          1     1      0   \n",
       "1                 1                 1              0          0     0      0   \n",
       "2                 0                 0              0          1     0      0   \n",
       "3                 0                 0              0          0     0      0   \n",
       "4                 0                 0              1          1     0      0   \n",
       "...             ...               ...            ...        ...   ...    ...   \n",
       "1587561           0                 0              0          0     0      0   \n",
       "1587562           0                 1              0          0     1      0   \n",
       "1587563           0                 0              1          0     0      0   \n",
       "1587564           0                 1              0          0     0      0   \n",
       "1587565           0                 0              0          1     0      0   \n",
       "\n",
       "         paragraph_symbol  hashtag  \\\n",
       "0                       0        0   \n",
       "1                       0        0   \n",
       "2                       0        0   \n",
       "3                       0        0   \n",
       "4                       0        0   \n",
       "...                   ...      ...   \n",
       "1587561                 0        0   \n",
       "1587562                 0        0   \n",
       "1587563                 0        0   \n",
       "1587564                 0        0   \n",
       "1587565                 0        1   \n",
       "\n",
       "                                    tweet_tokenized_string  \n",
       "0        awww thats bummer shoulda got david carr third...  \n",
       "1        upset cant update facebook texting might cry r...  \n",
       "2                many time ball managed save rest go bound  \n",
       "3                          whole body feel itchy like fire  \n",
       "4                                 behaving im mad cant see  \n",
       "...                                                    ...  \n",
       "1587561                      woke school best feeling ever  \n",
       "1587562                     cool hear old walt interview â  \n",
       "1587563                     ready mojo makeover ask detail  \n",
       "1587564              happy th birthday boo alll time tupac  \n",
       "1587565                                              happy  \n",
       "\n",
       "[1587566 rows x 22 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index löschen um ihn neu zu nummerieren da sich die Anzahl der Tweets geändert hat\n",
    "data = data.reset_index()\n",
    "data = data.drop(columns=['index'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1587566 entries, 0 to 1587565\n",
      "Data columns (total 22 columns):\n",
      " #   Column                  Non-Null Count    Dtype         \n",
      "---  ------                  --------------    -----         \n",
      " 0   sentiment               1587566 non-null  int64         \n",
      " 1   id                      1587566 non-null  int64         \n",
      " 2   user                    1587566 non-null  object        \n",
      " 3   tweet                   1587566 non-null  object        \n",
      " 4   tweet_tokenized         1587566 non-null  object        \n",
      " 5   date                    1587566 non-null  datetime64[ns]\n",
      " 6   year                    1587566 non-null  int64         \n",
      " 7   month                   1587566 non-null  int64         \n",
      " 8   weekday                 1587566 non-null  int64         \n",
      " 9   time                    1587566 non-null  object        \n",
      " 10  hour                    1587566 non-null  int64         \n",
      " 11  time_group              1587566 non-null  object        \n",
      " 12  word_count              1587566 non-null  int64         \n",
      " 13  dot_dot_dot             1587566 non-null  int64         \n",
      " 14  exclamation_mark        1587566 non-null  int64         \n",
      " 15  question_mark           1587566 non-null  int64         \n",
      " 16  at_symbol               1587566 non-null  int64         \n",
      " 17  link                    1587566 non-null  int64         \n",
      " 18  money                   1587566 non-null  int64         \n",
      " 19  paragraph_symbol        1587566 non-null  int64         \n",
      " 20  hashtag                 1587566 non-null  int64         \n",
      " 21  tweet_tokenized_string  1587566 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(15), object(6)\n",
      "memory usage: 266.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['id', 'user', 'tweet', 'tweet_tokenized', 'date', 'year', 'month', 'time', 'time_group'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('data_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data in Dependent and indepentent variable\n",
    "X = data.drop(columns=['sentiment'], axis = 1)\n",
    "y= data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2360 sha256=2e7b18ce16e9717573f7470118bce5107314cc122abcdb7fc9351dc7e6eb62b4\n",
      "  Stored in directory: c:\\users\\ollin\\appdata\\local\\pip\\cache\\wheels\\36\\49\\c9\\2374f1dee1b599effabf63d948635e6608f62d0ccde027b7e2\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split into 3 sets (Training, Validation and Test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation_and_test, y_train, y_validation_and_test = train_test_split(X, y, test_size=.1, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_validation_and_test, y_validation_and_test, test_size=.1, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1428809 entries with 50.06% negative, 49.94% positive\n",
      "Validation set has total 142881 entries with 49.89% negative, 50.11% positive\n",
      "Test set has total 15876 entries with 50.01% negative, 49.99% positive\n"
     ]
    }
   ],
   "source": [
    "print (\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_train),\n",
    "    (len(X_train[y_train == 0]) / (len(X_train)*1.))*100,\n",
    "    (len(X_train[y_train == 1]) / (len(X_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_validation),\n",
    "    (len(X_validation[y_validation == 0]) / (len(X_validation)*1.))*100,\n",
    "    (len(X_validation[y_validation == 1]) / (len(X_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_test),\n",
    "    (len(X_test[y_test == 0]) / (len(X_test)*1.))*100,\n",
    "    (len(X_test[y_test == 1]) / (len(X_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Baseline for comparison of Our Model\n",
    "#Use Textblob (Pre-Trained Model, in which you only feed the strings)\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Create DF to only feed in strings without any extra features\n",
    "X_validation_strings = X_validation[ 'tweet_tokenized_string']\n",
    "\n",
    "\n",
    "tbresult = [TextBlob(i).sentiment.polarity for i in X_validation_strings]\n",
    "tbpred = [0 if n < 0 else 1 for n in tbresult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 59.60%\n",
      "--------------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "\n",
      "          predicted_positive  predicted_negative\n",
      "positive               64136                7465\n",
      "negative               50263               21017\n"
     ]
    }
   ],
   "source": [
    "conmat = np.array(confusion_matrix(y_validation, tbpred, labels=[1,0]))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['positive', 'negative'],\n",
    "                         columns=['predicted_positive','predicted_negative'])\n",
    "\n",
    "print (\"Accuracy Score: {0:.2f}%\".format(accuracy_score(y_validation, tbpred)*100))\n",
    "print (\"-\"*80)\n",
    "print (\"Confusion Matrix\\n\")\n",
    "print (confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print \"accuracy score: {0:.2f}%\".format(accuracy*100)\n",
    "    print \"-\"*80\n",
    "    return accuracy, train_test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "additional_features_train = X_train[['year',\n",
    "                            'month',\n",
    "                            'weekday',\n",
    "                            'hour',\n",
    "                            'word_count',\n",
    "                            'dot_dot_dot',\n",
    "                            'exclamation_mark',\n",
    "                            'question_mark',\n",
    "                            'at_symbol',\n",
    "                            'link',\n",
    "                            'money',\n",
    "                            'paragraph_symbol',\n",
    "                            'hashtag'\n",
    "                            ]]\n",
    "\n",
    "# We convert the additional_features dataframe to a sparse matrix\n",
    "additional_features_train_matrix = scipy.sparse.csr_matrix(additional_features_train.values)\n",
    "\n",
    "# Then we horizontally stack the tf-idf matrix with the additional features\n",
    "X_train_matrix = hstack([X_train_vect_matrix, additional_features_train_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 75.57%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 74.85%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 75.54%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 75.64%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 75.53%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 74.68%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 74.68%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 74.68%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 74.68%\n",
      "--------------------------------------------------------------------------------\n",
      "accuracy score: 74.68%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression(random_state = 42)\n",
    "n_features = np.arange(10000,100001,10000)\n",
    "\n",
    "add_features_train = X_train[['weekday','hour','word_count','dot_dot_dot','exclamation_mark','question_mark','at_symbol','link','money','paragraph_symbol','hashtag']]\n",
    "add_features_test = X_test[['weekday','hour','word_count','dot_dot_dot','exclamation_mark','question_mark','at_symbol','link','money','paragraph_symbol','hashtag']]\n",
    "\n",
    "\n",
    "for n in n_features:\n",
    "\n",
    "    #Initialise Vectoriser in loop\n",
    "    cvec = CountVectorizer(max_features=n)\n",
    "\n",
    "    #Vectorize String\n",
    "    X_train_matrix = cvec.fit_transform(X_train['tweet_tokenized_string'])\n",
    "    X_test_matrix = cvec.transform(X_test['tweet_tokenized_string'])\n",
    "\n",
    "    # We convert the additional_features dataframe to a sparse matrix\n",
    "    add_features_train_matrix = scipy.sparse.csr_matrix(add_features_train.values)\n",
    "    add_features_test_matrix = scipy.sparse.csr_matrix(add_features_test.values)\n",
    "\n",
    "    # Then we horizontally stack the tf-idf matrix with the additional features\n",
    "    X_train_matrix = hstack([X_train_matrix, add_features_train_matrix])\n",
    "    X_test_matrix = hstack([X_test_matrix, add_features_test_matrix])\n",
    "\n",
    "    #fit(train) model\n",
    "    lr.fit(X_train_matrix, y_train)\n",
    "    y_pred = lr.predict(X_test_matrix)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    print (\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eigenes Modell bauen\n",
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ab hier im neuen Notebook weiterarbeiten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data[['sentiment', 'year', 'month', 'weekday', 'hour', 'word_count', 'dot_dot_dot', 'exclamation_mark', 'question_mark', 'at_symbol', 'link', 'money', 'paragraph_symbol', 'hashtag', 'tweet_tokenized_string']]\n",
    "model_data.head(2)\n",
    "X = model_data.drop(columns='sentiment')\n",
    "y = model_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1587566, 51602)\n",
      "['aa' 'aaa' 'aaaa' ... 'ùù' 'ùùù' 'ùùùù']\n"
     ]
    }
   ],
   "source": [
    "# Vectorizen mit allen Daten damit wir eine Matrix haben die alle Wörter enthält - Für das neuronale Netz müssen train und test Daten die gleichen Spalten enthalten. Sonst gibt es ein Problem bei Wörtern die nur in einem Split vorkommen. \n",
    "# Wir wollen das lösen indem wir eine Matrix aus dem gesamten Datensatz, also mit allen Wörtern, erstellen. Die Zeilen in diesen dann zu löschen und mit einer aus den Train Daten neu erstellten Matrix ersetzen. \n",
    "\n",
    "# Matrix aus allen Daten (Alle Wörter)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer() #Create a vectorizer that weighs the count of each token by its frequency in the dataset\n",
    "X_vect_matrix = tfidf_vect.fit_transform(X['tweet_tokenized_string']) #Create a matrix with the tfidf score of each token in each tweet\n",
    "print(X_vect_matrix.shape)\n",
    "print(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1587565x51602 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10367400 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Versuch die Zeilen der Matrix zu löschen aber die Spalten zu behalten. Hat aber nicht funktioniert. Wir wissen nicht ob die Spaltennamen in der sparse Matrix stehen.\n",
    "#Wenn das nicht der Fall wäre ist die Zuordnung der neuen Matrix zu der Matrix mit allen Wörtern schwierig\n",
    "\n",
    "# Delete all columns\n",
    "X_vect_matrix = X_vect_matrix.tocsc()  # Convert to CSC format for efficient column deletion\n",
    "X_vect_matrix = X_vect_matrix[:, :]\n",
    "X_vect_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dave\\VSCODEoderGIT\\MarketingAnalyticsProject\\marketing_analytics_project\\Notebooks\\data_preparation.ipynb Cell 33\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Dave/VSCODEoderGIT/MarketingAnalyticsProject/marketing_analytics_project/Notebooks/data_preparation.ipynb#Y165sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_train_vect_matrix_new \u001b[39m=\u001b[39m tfidf_vect\u001b[39m.\u001b[39;49mtransform(X_train)\n",
      "File \u001b[1;32mc:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2157\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2140\u001b[0m \u001b[39m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \n\u001b[0;32m   2142\u001b[0m \u001b[39mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2153\u001b[0m \u001b[39m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2154\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2155\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m, msg\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2157\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtransform(raw_documents)\n\u001b[0;32m   2158\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mtransform(X, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1433\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1430\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1432\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1433\u001b[0m _, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, fixed_vocab\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1434\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1435\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "File \u001b[1;32mc:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[0;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "#Hier würden wir eine neue Matrix aus den Train Daten erstellen um die Values dann in die Matrix mit allen Wörtern einfügen.\n",
    "X_train_vect_matrix_new = tfidf_vect.transform(X_train)\n",
    "from scipy.sparse import csr_matrix\n",
    "zero_matrix = csr_matrix(X_train_vect_matrix_new.shape, dtype=np.float64)\n",
    "index_array = np.arange(X_train_vect_matrix.shape[0])\n",
    "\n",
    "# Align the shapes of the matrices and assign values\n",
    "zero_matrix[np.ix_(index_array[X_train.index], np.arange(X_train_vect_matrix.shape[1]))] = X_train_vect_matrix_new\n",
    "\n",
    "# Print the shape and feature names of the new matrix\n",
    "print(zero_matrix.shape)\n",
    "print(tfidf_vect.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1063669, 51551)\n",
      "['aa' 'aaa' 'aaaa' ... 'ùøùùù' 'ùù' 'ùùù']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer() #Create a vectorizer that weighs the count of each token by its frequency in the dataset\n",
    "X_train_vect_matrix = tfidf_vect.fit_transform(X_train['tweet_tokenized_string']) #Create a matrix with the tfidf score of each token in each tweet\n",
    "print(X_train_vect_matrix.shape)\n",
    "print(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So können wir später unsere erstellten Features an die Matrix anhängen - Einmal für die Train und die Test Matrix\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "additional_features_train = X_train[['year',\n",
    "                            'month',\n",
    "                            'weekday',\n",
    "                            'hour',\n",
    "                            'word_count',\n",
    "                            'dot_dot_dot',\n",
    "                            'exclamation_mark',\n",
    "                            'question_mark',\n",
    "                            'at_symbol',\n",
    "                            'link',\n",
    "                            'money',\n",
    "                            'paragraph_symbol',\n",
    "                            'hashtag'\n",
    "                            ]]\n",
    "\n",
    "# We convert the additional_features dataframe to a sparse matrix\n",
    "additional_features_train_matrix = scipy.sparse.csr_matrix(additional_features_train.values)\n",
    "\n",
    "# Then we horizontally stack the tf-idf matrix with the additional features\n",
    "X_train_matrix = hstack([X_train_vect_matrix, additional_features_train_matrix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tf-idf matrix shape: (1063669, 51551)\n",
      "Combined matrix shape: (1063669, 51564)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tf-idf matrix shape:\", X_train_vect_matrix.shape)\n",
    "print(\"Combined matrix shape:\", X_train_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(523897, 49645)\n",
      "['aa' 'aaa' 'aaaa' ... 'ùù' 'ùùù' 'ùùùù']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer() #Create a vectorizer that weighs the count of each token by its frequency in the dataset\n",
    "X_test_vect_matrix = tfidf_vect.fit_transform(X_test['tweet_tokenized_string']) #Create a matrix with the tfidf score of each token in each tweet\n",
    "print(X_test_vect_matrix.shape)\n",
    "print(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "additional_features_test = X_test[['year',\n",
    "                            'month',\n",
    "                            'weekday',\n",
    "                            'hour',\n",
    "                            'word_count',\n",
    "                            'dot_dot_dot',\n",
    "                            'exclamation_mark',\n",
    "                            'question_mark',\n",
    "                            'at_symbol',\n",
    "                            'link',\n",
    "                            'money',\n",
    "                            'paragraph_symbol',\n",
    "                            'hashtag'\n",
    "                            ]]\n",
    "\n",
    "# We convert the additional_features dataframe to a sparse matrix\n",
    "additional_features_test_matrix = scipy.sparse.csr_matrix(additional_features_test.values)\n",
    "\n",
    "# Then we horizontally stack the tf-idf matrix with the additional features\n",
    "X_test_matrix = hstack([X_test_vect_matrix, additional_features_test_matrix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tf-idf matrix shape: (523897, 49645)\n",
      "Combined matrix shape: (523897, 49658)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tf-idf matrix shape:\", X_test_vect_matrix.shape)\n",
    "print(\"Combined matrix shape:\", X_test_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49658"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# assuming your input data has 100000 features\n",
    "input_dim = X_train_matrix.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=input_dim, activation='relu'))  # Hidden layer 1\n",
    "model.add(Dense(128, activation='relu'))  # Hidden layer 2\n",
    "model.add(Dense(64, activation='relu'))  # Hidden layer 3\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[2] = [0,25425] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Dave\\VSCODEoderGIT\\MarketingAnalyticsProject\\marketing_analytics_project\\Notebooks\\data_preparation.ipynb Cell 39\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Dave/VSCODEoderGIT/MarketingAnalyticsProject/marketing_analytics_project/Notebooks/data_preparation.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m neural_network \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train_matrix, y_train_matrix, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m) \u001b[39m#validation_data=(X_test_matrix, y_test))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__SerializeManySparse_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[2] = [0,25425] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]"
     ]
    }
   ],
   "source": [
    "neural_network = model.fit(X_train_matrix, y_train, epochs=10, batch_size=64) #validation_data=(X_test_matrix, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing first"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uni/Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweet_tokenized_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[many, time, ball, managed, save, rest, go, bo...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>many time ball managed save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>[cool, hear, old, walt, interview, â]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>cool hear old walt interview â</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>[ready, mojo, makeover, ask, detail]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>[happy, th, birthday, boo, alll, time, tupac]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>happy th birthday boo alll time tupac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:50</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1587566 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id             user  \\\n",
       "0                0  1467810369  _TheSpecialOne_   \n",
       "1                0  1467810672    scotthamilton   \n",
       "2                0  1467810917         mattycus   \n",
       "3                0  1467811184          ElleCTF   \n",
       "4                0  1467811193           Karoli   \n",
       "...            ...         ...              ...   \n",
       "1599995          1  2193601966  AmandaMarie1028   \n",
       "1599996          1  2193601969      TheWDBoards   \n",
       "1599997          1  2193601991           bpbabe   \n",
       "1599998          1  2193602064     tinydiamondz   \n",
       "1599999          1  2193602129   RyanTrevMorris   \n",
       "\n",
       "                                                     tweet  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1        is upset that he can't update his Facebook by ...   \n",
       "2        @Kenichan I dived many times for the ball. Man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1599995  Just woke up. Having no school is the best fee...   \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                           tweet_tokenized  \\\n",
       "0        [awww, thats, bummer, shoulda, got, david, car...   \n",
       "1        [upset, cant, update, facebook, texting, might...   \n",
       "2        [many, time, ball, managed, save, rest, go, bo...   \n",
       "3                   [whole, body, feel, itchy, like, fire]   \n",
       "4                           [behaving, im, mad, cant, see]   \n",
       "...                                                    ...   \n",
       "1599995                [woke, school, best, feeling, ever]   \n",
       "1599996              [cool, hear, old, walt, interview, â]   \n",
       "1599997               [ready, mojo, makeover, ask, detail]   \n",
       "1599998      [happy, th, birthday, boo, alll, time, tupac]   \n",
       "1599999                                            [happy]   \n",
       "\n",
       "                       date  year  month  weekday      time  ...  word_count  \\\n",
       "0       2009-04-06 22:19:45  2009      4        0  22:19:45  ...          19   \n",
       "1       2009-04-06 22:19:49  2009      4        0  22:19:49  ...          21   \n",
       "2       2009-04-06 22:19:53  2009      4        0  22:19:53  ...          18   \n",
       "3       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          10   \n",
       "4       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          21   \n",
       "...                     ...   ...    ...      ...       ...  ...         ...   \n",
       "1599995 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599996 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599997 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599998 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          12   \n",
       "1599999 2009-06-16 08:40:50  2009      6        1  08:40:50  ...           5   \n",
       "\n",
       "        dot_dot_dot  exclamation_mark  question_mark  at_symbol   link  money  \\\n",
       "0             False             False          False       True   True  False   \n",
       "1              True              True          False      False  False  False   \n",
       "2             False             False          False       True  False  False   \n",
       "3             False             False          False      False  False  False   \n",
       "4             False             False           True       True  False  False   \n",
       "...             ...               ...            ...        ...    ...    ...   \n",
       "1599995       False             False          False      False  False  False   \n",
       "1599996       False              True          False      False   True  False   \n",
       "1599997       False             False           True      False  False  False   \n",
       "1599998       False              True          False      False  False  False   \n",
       "1599999       False             False          False       True  False  False   \n",
       "\n",
       "         paragraph_symbol  hashtag  \\\n",
       "0                   False    False   \n",
       "1                   False    False   \n",
       "2                   False    False   \n",
       "3                   False    False   \n",
       "4                   False    False   \n",
       "...                   ...      ...   \n",
       "1599995             False    False   \n",
       "1599996             False    False   \n",
       "1599997             False    False   \n",
       "1599998             False    False   \n",
       "1599999             False     True   \n",
       "\n",
       "                                    tweet_tokenized_string  \n",
       "0        awww thats bummer shoulda got david carr third...  \n",
       "1        upset cant update facebook texting might cry r...  \n",
       "2                many time ball managed save rest go bound  \n",
       "3                          whole body feel itchy like fire  \n",
       "4                                 behaving im mad cant see  \n",
       "...                                                    ...  \n",
       "1599995                      woke school best feeling ever  \n",
       "1599996                     cool hear old walt interview â  \n",
       "1599997                     ready mojo makeover ask detail  \n",
       "1599998              happy th birthday boo alll time tupac  \n",
       "1599999                                              happy  \n",
       "\n",
       "[1587566 rows x 22 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1587566, 3395516)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ngram_vect = CountVectorizer(ngram_range=(1,2)) #Create a vectorizer that creates unigrams and bigrams from our tweets\n",
    "ngram_matrix = ngram_vect.fit_transform(data['tweet_tokenized_string']) #Create a matrix with the counts of each unigram and bigram in each tweet\n",
    "print(ngram_matrix.shape) #Print the shape of the matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1587566x3395516 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19376522 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aa  aa allstars  aa already  aa amp  aa arrive  aa ba  aa bachelor  aa bad  \\\n",
      "0   0            0           0       0          0      0            0       0   \n",
      "1   0            0           0       0          0      0            0       0   \n",
      "2   0            0           0       0          0      0            0       0   \n",
      "3   0            0           0       0          0      0            0       0   \n",
      "4   0            0           0       0          0      0            0       0   \n",
      "5   0            0           0       0          0      0            0       0   \n",
      "6   0            0           0       0          0      0            0       0   \n",
      "7   0            0           0       0          0      0            0       0   \n",
      "8   0            0           0       0          0      0            0       0   \n",
      "9   0            0           0       0          0      0            0       0   \n",
      "\n",
      "   aa baseball  aa battery  \n",
      "0            0           0  \n",
      "1            0           0  \n",
      "2            0           0  \n",
      "3            0           0  \n",
      "4            0           0  \n",
      "5            0           0  \n",
      "6            0           0  \n",
      "7            0           0  \n",
      "8            0           0  \n",
      "9            0           0  \n"
     ]
    }
   ],
   "source": [
    "#Look at a sample of the created ngram matrix to see how it looks like\n",
    "ngram_matrix_sample = ngram_matrix[:10, :10]\n",
    "feature_names = ngram_vect.get_feature_names_out()[:10]\n",
    "ngram_matrix_sample = ngram_matrix_sample.toarray()\n",
    "ngram_matrix_sample_df = pd.DataFrame(ngram_matrix_sample, columns=feature_names)\n",
    "print(ngram_matrix_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aa allstars' 'aa already' ... 'ùùù ùù' 'ùùùù' 'ùùùù gtlt']\n"
     ]
    }
   ],
   "source": [
    "print(ngram_vect.get_feature_names_out()) #Print the names of the unigrams and bigrams\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1587566, 51602)\n",
      "['aa' 'aaa' 'aaaa' ... 'ùù' 'ùùù' 'ùùùù']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer() #Create a vectorizer that weighs the count of each token by its frequency in the dataset\n",
    "tfidf_matrix = tfidf_vect.fit_transform(data['tweet_tokenized_string']) #Create a matrix with the tfidf score of each token in each tweet\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    aa  aaa  aaaa  aaaaa  aaaaaa  aaaaaaa  aaaaaaaa  aaaaaaaaa  aaaaaaaaaa  \\\n",
      "0  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "1  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "2  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "3  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "4  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "5  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "6  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "7  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "8  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "9  0.0  0.0   0.0    0.0     0.0      0.0       0.0        0.0         0.0   \n",
      "\n",
      "   aaaaaaaaaaaaa  \n",
      "0            0.0  \n",
      "1            0.0  \n",
      "2            0.0  \n",
      "3            0.0  \n",
      "4            0.0  \n",
      "5            0.0  \n",
      "6            0.0  \n",
      "7            0.0  \n",
      "8            0.0  \n",
      "9            0.0  \n"
     ]
    }
   ],
   "source": [
    "#Look at a sample of the created tdidf matrix to see how it looks like\n",
    "tfidf_matrix_sample = tfidf_matrix[:10, :10]\n",
    "feature_names = tfidf_vect.get_feature_names_out()[:10]\n",
    "tfidf_matrix_sample = tfidf_matrix_sample.toarray()\n",
    "tfidf_matrix_sample_df = pd.DataFrame(tfidf_matrix_sample, columns=feature_names)\n",
    "print(tfidf_matrix_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\dave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in c:\\users\\dave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scipy) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Dave\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Suppose df['contains_exclamation'] and df['num_words'] are your additional features\n",
    "additional_features = data[['sentiment',\n",
    "                            'year',\n",
    "                            'month',\n",
    "                            'weekday',\n",
    "                            'hour',\n",
    "                            'word_count',\n",
    "                            'dot_dot_dot',\n",
    "                            'exclamation_mark',\n",
    "                            'question_mark',\n",
    "                            'at_symbol',\n",
    "                            'link',\n",
    "                            'money',\n",
    "                            'paragraph_symbol',\n",
    "                            'hashtag'\n",
    "                            ]]\n",
    "\n",
    "# We convert the additional_features dataframe to a sparse matrix\n",
    "additional_features_sparse = scipy.sparse.csr_matrix(additional_features.values)\n",
    "\n",
    "# Then we horizontally stack the tf-idf matrix with the additional features\n",
    "X_combined = hstack([tfidf_matrix, additional_features_sparse])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tf-idf matrix shape: (1587566, 51602)\n",
      "Combined matrix shape: (1587566, 51616)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tf-idf matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"Combined matrix shape:\", X_combined.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
