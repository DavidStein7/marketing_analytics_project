{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ollin\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read csv file and name columns\n",
    "data = pd.read_csv(\"../Data/Twitter.csv\", header=None, encoding='latin-1')\n",
    "data.columns = ['sentiment', 'id', 'date', 'query', 'user', 'tweet']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date             user  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
       "\n",
       "                                               tweet  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the column 'query', as it only contains 'NO_QUERY'\n",
    "data = data.drop(columns= 'query')\n",
    "\n",
    "#Replace the 4 for a positive sentiment with a 1 for easier understanding (there are no numbers between 0 and 4)\n",
    "data['sentiment'] = data['sentiment'].replace(4, 1)\n",
    "#0 = negative, 1 = positive\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>time_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet                date  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t... 2009-04-06 22:19:45   \n",
       "1  is upset that he can't update his Facebook by ... 2009-04-06 22:19:49   \n",
       "2  @Kenichan I dived many times for the ball. Man... 2009-04-06 22:19:53   \n",
       "3    my whole body feels itchy and like its on fire  2009-04-06 22:19:57   \n",
       "4  @nationwideclass no, it's not behaving at all.... 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  hour time_group  \n",
       "0  2009      4        0  22:19:45    22      20-24  \n",
       "1  2009      4        0  22:19:49    22      20-24  \n",
       "2  2009      4        0  22:19:53    22      20-24  \n",
       "3  2009      4        0  22:19:57    22      20-24  \n",
       "4  2009      4        0  22:19:57    22      20-24  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert Date and time column into datetime format (By stripping day, month, year and time manually as strings and passing them into datetime function)\n",
    "data['date_new'] = data['date'].str[8:10] + \"/\" + data['date'].str[4:7] + \"/\" + data['date'].str[24:28] + \", \" + data['date'].str[11:19]\n",
    "data['date_new'] = pd.to_datetime(data['date_new'], format=\"%d/%b/%Y, %H:%M:%S\")\n",
    "\n",
    "#extract DateTime information from column\n",
    "data['year'] = pd.DatetimeIndex(data['date_new']).year\n",
    "data['month'] = pd.DatetimeIndex(data['date_new']).month\n",
    "\n",
    "#Weekday where 0 = Monday and 6 = Sunday\n",
    "data['weekday'] = pd.DatetimeIndex(data['date_new']).weekday\n",
    "data['time'] = pd.DatetimeIndex(data['date_new']).time\n",
    "data['hour'] = pd.DatetimeIndex(data['date_new']).hour\n",
    "\n",
    "# Extract Timezones\n",
    "data['date'] = data['date'].astype('string')\n",
    "data['timezone'] = data['date'].str[20:23]\n",
    "data.head()\n",
    "\n",
    "##sort time into groups\n",
    "#create list of conditions (time groups)\n",
    "conditions = [\n",
    "    (data['hour'] < 4),\n",
    "    (data['hour'] >= 4) & (data['hour'] < 8),\n",
    "    (data['hour'] >= 8) & (data['hour'] < 12),\n",
    "    (data['hour'] >= 12) & (data['hour'] < 16),\n",
    "    (data['hour'] >= 16) & (data['hour'] < 20),\n",
    "    (data['hour'] >= 20)\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = ['0-4', '4-8', '8-12', '12-16', '16-20', '20-24']\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "data['time_group'] = np.select(conditions, values)\n",
    "\n",
    "#drop old date column\n",
    "data.drop(['date', 'timezone'], axis = 1, inplace=True)\n",
    "data.rename(columns={'date_new': 'date'}, inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>22</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet                date  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t... 2009-04-06 22:19:45   \n",
       "1  is upset that he can't update his Facebook by ... 2009-04-06 22:19:49   \n",
       "2  @Kenichan I dived many times for the ball. Man... 2009-04-06 22:19:53   \n",
       "3    my whole body feels itchy and like its on fire  2009-04-06 22:19:57   \n",
       "4  @nationwideclass no, it's not behaving at all.... 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  hour time_group  word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45    22      20-24          19        False   \n",
       "1  2009      4        0  22:19:49    22      20-24          21         True   \n",
       "2  2009      4        0  22:19:53    22      20-24          18        False   \n",
       "3  2009      4        0  22:19:57    22      20-24          10        False   \n",
       "4  2009      4        0  22:19:57    22      20-24          21        False   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol   link  money  paragraph_symbol  \\\n",
       "0             False          False       True   True  False             False   \n",
       "1              True          False      False  False  False             False   \n",
       "2             False          False       True  False  False             False   \n",
       "3             False          False      False  False  False             False   \n",
       "4             False           True       True  False  False             False   \n",
       "\n",
       "   hashtag  \n",
       "0    False  \n",
       "1    False  \n",
       "2    False  \n",
       "3    False  \n",
       "4    False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count the number of words per tweet\n",
    "data['word_count'] = data['tweet'].str.split().str.len()\n",
    "\n",
    "#Check, if certain special characters occur in a tweet (one-hot encoded)\n",
    "data['dot_dot_dot'] = data['tweet'].str.contains('\\.\\.\\.')\n",
    "data['exclamation_mark'] = data['tweet'].str.contains('!')\n",
    "data['question_mark'] = data['tweet'].str.contains('\\?')\n",
    "data['at_symbol'] = data['tweet'].str.contains('\\@')\n",
    "data['link'] = data['tweet'].str.contains('http')\n",
    "data['money'] = data['tweet'].str.contains('\\$|\\€|\\£')\n",
    "data['paragraph_symbol'] = data['tweet'].str.contains('\\§')\n",
    "data['hashtag'] = data['tweet'].str.contains('#')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {'dot_dot_dot': 'int64',\n",
    "                'exclamation_mark': 'int64',\n",
    "                'question_mark': 'int64',\n",
    "                'at_symbol': 'int64',\n",
    "                'link': 'int64',\n",
    "                'money': 'int64',\n",
    "                'paragraph_symbol': 'int64',\n",
    "                'hashtag': 'int64'\n",
    "                }\n",
    "data = data.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english') #Create a list of english stopwords from nltk\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_clean = [word.replace(\"'\", \"\") for word in stopword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the typical pattern of links ('https://', 'http://', 'www.'), tags ('@') and hashtags ('#')\n",
    "url_pattern_1 = r'https?://\\S+'\n",
    "url_pattern_2 = r'www\\.\\S+'\n",
    "tag_pattern = r'@\\S+'\n",
    "hashtag_pattern = r'#\\S+'\n",
    "\n",
    "#Add a new column for the tokenized tweet and remove all links, tags and hashtags from the tweets\n",
    "data.insert(4, 'tweet_tokenized', data['tweet'].apply(lambda x: re.sub(url_pattern_1, '', x)))\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: re.sub(url_pattern_2, '', x))\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: re.sub(tag_pattern, '', x))\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: re.sub(hashtag_pattern, '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>- Awww, that's a bummer.  You shoulda got Da...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>I dived many times for the ball. Managed to s...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am...</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>not the whole crew</td>\n",
       "      <td>2009-04-06 22:20:00</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:20:00</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "5          0  1467811372         joy_wolf   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "5                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0    - Awww, that's a bummer.  You shoulda got Da... 2009-04-06 22:19:45   \n",
       "1  is upset that he can't update his Facebook by ... 2009-04-06 22:19:49   \n",
       "2   I dived many times for the ball. Managed to s... 2009-04-06 22:19:53   \n",
       "3    my whole body feels itchy and like its on fire  2009-04-06 22:19:57   \n",
       "4   no, it's not behaving at all. i'm mad. why am... 2009-04-06 22:19:57   \n",
       "5                                not the whole crew  2009-04-06 22:20:00   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "5  2009      4        0  22:20:00  ...       20-24          5            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "5                 0              0          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "5        0  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handling Emojis\n",
    "# Function for converting emojis into word\n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = text.replace(emot, \"\".join(EMOTICONS_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text\n",
    "\n",
    "# Apply Formula to tweets\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: convert_emoticons(x))\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>- awww, that's a bummer.  you shoulda got davi...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0  - awww, that's a bummer.  you shoulda got davi... 2009-04-06 22:19:45   \n",
       "1  is upset that he can't update his facebook by ... 2009-04-06 22:19:49   \n",
       "2  i dived many times for the ball. managed to sa... 2009-04-06 22:19:53   \n",
       "3     my whole body feels itchy and like its on fire 2009-04-06 22:19:57   \n",
       "4  no, it's not behaving at all. i'm mad. why am ... 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove all punctuation from the tweets\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].str.replace('[^\\w\\s]', '')\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].str.replace('_', '')\n",
    "\n",
    "#Remove all whitespaces from the beginning or end of the tweets\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].str.strip()\n",
    "\n",
    "#Set all characters to lowercase\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: x.lower())\n",
    "\n",
    "#Remove all numbers\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].str.replace('\\d+', '')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[, awww, that, s, a, bummer, you, shoulda, got...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[is, upset, that, he, can, t, update, his, fac...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[no, it, s, not, behaving, at, all, i, m, mad,...</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0  [, awww, that, s, a, bummer, you, shoulda, got... 2009-04-06 22:19:45   \n",
       "1  [is, upset, that, he, can, t, update, his, fac... 2009-04-06 22:19:49   \n",
       "2  [i, dived, many, times, for, the, ball, manage... 2009-04-06 22:19:53   \n",
       "3  [my, whole, body, feels, itchy, and, like, its... 2009-04-06 22:19:57   \n",
       "4  [no, it, s, not, behaving, at, all, i, m, mad,... 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize sentences based on non-alphanumeric characters (Leerstelle)\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: tokenize(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[, awww, bummer, shoulda, got, david, carr, th...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, update, facebook, texting, might, cry,...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, 50, ...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, mad, see, ]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0  [, awww, bummer, shoulda, got, david, carr, th... 2009-04-06 22:19:45   \n",
       "1  [upset, update, facebook, texting, might, cry,... 2009-04-06 22:19:49   \n",
       "2  [dived, many, times, ball, managed, save, 50, ... 2009-04-06 22:19:53   \n",
       "3            [whole, body, feels, itchy, like, fire] 2009-04-06 22:19:57   \n",
       "4                             [behaving, mad, see, ] 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list): #Function to remove all stopword from our list of tokenized tweets\n",
    "    text = [word for word in tokenized_list if word not in stopword_clean] #Write each word from our tokenized list into a new list, if it is not in the stopword list\n",
    "    return text\n",
    "\n",
    "#Create new column with tokenized tweets without stopwords\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: remove_stopwords(x)) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>time_group</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[, awww, bummer, shoulda, got, david, carr, th...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, update, facebook, texting, might, cry,...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dived, many, time, ball, managed, save, 50, r...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, mad, see, ]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>20-24</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id             user  \\\n",
       "0          0  1467810369  _TheSpecialOne_   \n",
       "1          0  1467810672    scotthamilton   \n",
       "2          0  1467810917         mattycus   \n",
       "3          0  1467811184          ElleCTF   \n",
       "4          0  1467811193           Karoli   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                     tweet_tokenized                date  \\\n",
       "0  [, awww, bummer, shoulda, got, david, carr, th... 2009-04-06 22:19:45   \n",
       "1  [upset, update, facebook, texting, might, cry,... 2009-04-06 22:19:49   \n",
       "2  [dived, many, time, ball, managed, save, 50, r... 2009-04-06 22:19:53   \n",
       "3             [whole, body, feel, itchy, like, fire] 2009-04-06 22:19:57   \n",
       "4                             [behaving, mad, see, ] 2009-04-06 22:19:57   \n",
       "\n",
       "   year  month  weekday      time  ...  time_group word_count  dot_dot_dot  \\\n",
       "0  2009      4        0  22:19:45  ...       20-24         19            0   \n",
       "1  2009      4        0  22:19:49  ...       20-24         21            1   \n",
       "2  2009      4        0  22:19:53  ...       20-24         18            0   \n",
       "3  2009      4        0  22:19:57  ...       20-24         10            0   \n",
       "4  2009      4        0  22:19:57  ...       20-24         21            0   \n",
       "\n",
       "   exclamation_mark  question_mark  at_symbol  link  money  paragraph_symbol  \\\n",
       "0                 0              0          1     1      0                 0   \n",
       "1                 1              0          0     0      0                 0   \n",
       "2                 0              0          1     0      0                 0   \n",
       "3                 0              0          0     0      0                 0   \n",
       "4                 0              1          1     0      0                 0   \n",
       "\n",
       "   hashtag  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "wnlemm = nltk.WordNetLemmatizer() \n",
    "\n",
    "def lemmatizing(tokenized_text): #Function to lemmatize all words in our tokenized tweets list without stopwords\n",
    "    text = [wnlemm.lemmatize(word) for word in tokenized_text] #Lemmatize each word in our tokenized list and write it into a new list\n",
    "    return text\n",
    "\n",
    "#Create new column with lemmatized tweets from our tokenized tweets without stopwords\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(lambda x: lemmatizing(x))\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweet_tokenized_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[, awww, bummer, shoulda, got, david, carr, th...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>awww bummer shoulda got david carr third day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, update, facebook, texting, might, cry,...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[dived, many, time, ball, managed, save, 50, r...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dived many time ball managed save 50 rest go b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, mad, see, ]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>[thewdb, com, cool, hear, old, walt, interview...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thewdb com cool hear old walt interview â</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>[ready, mojo, makeover, ask, detail]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>[happy, 38th, birthday, boo, alll, time, tupac...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happy 38th birthday boo alll time tupac amaru ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:50</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id             user  \\\n",
       "0                0  1467810369  _TheSpecialOne_   \n",
       "1                0  1467810672    scotthamilton   \n",
       "2                0  1467810917         mattycus   \n",
       "3                0  1467811184          ElleCTF   \n",
       "4                0  1467811193           Karoli   \n",
       "...            ...         ...              ...   \n",
       "1599995          1  2193601966  AmandaMarie1028   \n",
       "1599996          1  2193601969      TheWDBoards   \n",
       "1599997          1  2193601991           bpbabe   \n",
       "1599998          1  2193602064     tinydiamondz   \n",
       "1599999          1  2193602129   RyanTrevMorris   \n",
       "\n",
       "                                                     tweet  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1        is upset that he can't update his Facebook by ...   \n",
       "2        @Kenichan I dived many times for the ball. Man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1599995  Just woke up. Having no school is the best fee...   \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                           tweet_tokenized  \\\n",
       "0        [, awww, bummer, shoulda, got, david, carr, th...   \n",
       "1        [upset, update, facebook, texting, might, cry,...   \n",
       "2        [dived, many, time, ball, managed, save, 50, r...   \n",
       "3                   [whole, body, feel, itchy, like, fire]   \n",
       "4                                   [behaving, mad, see, ]   \n",
       "...                                                    ...   \n",
       "1599995                [woke, school, best, feeling, ever]   \n",
       "1599996  [thewdb, com, cool, hear, old, walt, interview...   \n",
       "1599997               [ready, mojo, makeover, ask, detail]   \n",
       "1599998  [happy, 38th, birthday, boo, alll, time, tupac...   \n",
       "1599999                                            [happy]   \n",
       "\n",
       "                       date  year  month  weekday      time  ...  word_count  \\\n",
       "0       2009-04-06 22:19:45  2009      4        0  22:19:45  ...          19   \n",
       "1       2009-04-06 22:19:49  2009      4        0  22:19:49  ...          21   \n",
       "2       2009-04-06 22:19:53  2009      4        0  22:19:53  ...          18   \n",
       "3       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          10   \n",
       "4       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          21   \n",
       "...                     ...   ...    ...      ...       ...  ...         ...   \n",
       "1599995 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599996 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599997 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599998 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          12   \n",
       "1599999 2009-06-16 08:40:50  2009      6        1  08:40:50  ...           5   \n",
       "\n",
       "        dot_dot_dot  exclamation_mark  question_mark  at_symbol  link  money  \\\n",
       "0                 0                 0              0          1     1      0   \n",
       "1                 1                 1              0          0     0      0   \n",
       "2                 0                 0              0          1     0      0   \n",
       "3                 0                 0              0          0     0      0   \n",
       "4                 0                 0              1          1     0      0   \n",
       "...             ...               ...            ...        ...   ...    ...   \n",
       "1599995           0                 0              0          0     0      0   \n",
       "1599996           0                 1              0          0     1      0   \n",
       "1599997           0                 0              1          0     0      0   \n",
       "1599998           0                 1              0          0     0      0   \n",
       "1599999           0                 0              0          1     0      0   \n",
       "\n",
       "         paragraph_symbol  hashtag  \\\n",
       "0                       0        0   \n",
       "1                       0        0   \n",
       "2                       0        0   \n",
       "3                       0        0   \n",
       "4                       0        0   \n",
       "...                   ...      ...   \n",
       "1599995                 0        0   \n",
       "1599996                 0        0   \n",
       "1599997                 0        0   \n",
       "1599998                 0        0   \n",
       "1599999                 0        1   \n",
       "\n",
       "                                    tweet_tokenized_string  \n",
       "0         awww bummer shoulda got david carr third day ...  \n",
       "1        upset update facebook texting might cry result...  \n",
       "2        dived many time ball managed save 50 rest go b...  \n",
       "3                          whole body feel itchy like fire  \n",
       "4                                        behaving mad see   \n",
       "...                                                    ...  \n",
       "1599995                      woke school best feeling ever  \n",
       "1599996         thewdb com cool hear old walt interview â   \n",
       "1599997                     ready mojo makeover ask detail  \n",
       "1599998  happy 38th birthday boo alll time tupac amaru ...  \n",
       "1599999                                              happy  \n",
       "\n",
       "[1600000 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N-gram vectorizing and tfidf need a list of strings passed to it, so we need to convert our list\n",
    "data['tweet_tokenized_string'] = data['tweet_tokenized'].apply(lambda x: ' '.join(x)) #Join each word in our list with a space inbetween\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(data.tweet_tokenized_string.str.split(expand=True).stack().value_counts()).reset_index()\n",
    "words.rename(columns={'index': 'word', 0: 'count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48626</th>\n",
       "      <td>hih</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48627</th>\n",
       "      <td>euna</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48628</th>\n",
       "      <td>deat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48629</th>\n",
       "      <td>hwz</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48630</th>\n",
       "      <td>anyfin</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269068</th>\n",
       "      <td>agingforwards</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269069</th>\n",
       "      <td>fromme</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269070</th>\n",
       "      <td>numbre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269071</th>\n",
       "      <td>deppresing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269072</th>\n",
       "      <td>thewdb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220447 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  count\n",
       "48626             hih      4\n",
       "48627            euna      4\n",
       "48628            deat      4\n",
       "48629             hwz      4\n",
       "48630          anyfin      4\n",
       "...               ...    ...\n",
       "269068  agingforwards      1\n",
       "269069         fromme      1\n",
       "269070         numbre      1\n",
       "269071     deppresing      1\n",
       "269072         thewdb      1\n",
       "\n",
       "[220447 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_split = words[words['count'] < 5]\n",
    "words_to_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hih',\n",
       " 'euna',\n",
       " 'deat',\n",
       " 'hwz',\n",
       " 'anyfin',\n",
       " 'launchpad',\n",
       " 'dlt',\n",
       " 'kelci',\n",
       " 'vapid',\n",
       " 'earmuff',\n",
       " 'itched',\n",
       " 'berserker',\n",
       " 'rumi',\n",
       " 'commish',\n",
       " 'napper',\n",
       " 'crank2',\n",
       " 'wonderul',\n",
       " 'healthily',\n",
       " 'compras',\n",
       " 'sholat',\n",
       " 'pyscho',\n",
       " 'engish',\n",
       " '5hour',\n",
       " 'ohgosh',\n",
       " 'unles',\n",
       " 'stevenage',\n",
       " 'christinas',\n",
       " 'relaxxx',\n",
       " 'nlang',\n",
       " '2late',\n",
       " 'homieee',\n",
       " 'histology',\n",
       " 'chlamydia',\n",
       " 'nawws',\n",
       " 'bergman',\n",
       " 'gracia',\n",
       " 'stumptown',\n",
       " 'reverend',\n",
       " 'sabes',\n",
       " 'quotwinkorsmirkat',\n",
       " 'fufilled',\n",
       " 'trc',\n",
       " 'ilysfm',\n",
       " 'danelle',\n",
       " 'crusted',\n",
       " 'enacting',\n",
       " 'conect',\n",
       " 'goede',\n",
       " 'fucksticks',\n",
       " 'jamacia',\n",
       " 'oowwww',\n",
       " 'lovvveeee',\n",
       " 'thinx',\n",
       " 'sueno',\n",
       " 'dufferin',\n",
       " 'masonic',\n",
       " 'pressin',\n",
       " '½we',\n",
       " 'bennington',\n",
       " 'masina',\n",
       " 'yiayia',\n",
       " 'j1',\n",
       " 'urrrrgh',\n",
       " 'followill',\n",
       " 'monavie',\n",
       " 'alices',\n",
       " 'klatch',\n",
       " 'demoscene',\n",
       " 'abscence',\n",
       " 'dona',\n",
       " 'ohhkay',\n",
       " 'bottleneck',\n",
       " 'doohickey',\n",
       " 'fugged',\n",
       " 'yuppy',\n",
       " 'oneal',\n",
       " 'muwah',\n",
       " 'souped',\n",
       " 'jal',\n",
       " 'scoreeeee',\n",
       " 'qosh',\n",
       " 'papou',\n",
       " 'ohmahgah',\n",
       " '½ä',\n",
       " 'condoning',\n",
       " 'speculate',\n",
       " 'nesn',\n",
       " 'windpipe',\n",
       " 'alumna',\n",
       " 'itchiness',\n",
       " '4b',\n",
       " 'jef',\n",
       " 'dubliner',\n",
       " 'fitflops',\n",
       " 'wilkins',\n",
       " 'niga',\n",
       " 'bluesy',\n",
       " 'aall',\n",
       " 'kraut',\n",
       " 'schlepping',\n",
       " 'liberally',\n",
       " 'ultima',\n",
       " 'certifiable',\n",
       " 'ohmygoodness',\n",
       " 'chungs',\n",
       " 'ex1',\n",
       " 'gutair',\n",
       " 'protestant',\n",
       " 'tanty',\n",
       " 'tetchy',\n",
       " 'balamory',\n",
       " 'calibrated',\n",
       " 'oyea',\n",
       " 'happee',\n",
       " 'sucat',\n",
       " 'randys',\n",
       " 'plumbob',\n",
       " 'retrieval',\n",
       " 'sona',\n",
       " 'yeat',\n",
       " 'dek',\n",
       " 'brenton',\n",
       " 'oldddd',\n",
       " 'winfrey',\n",
       " 'sosurprised',\n",
       " 'bergerac',\n",
       " 'sume',\n",
       " 'stomack',\n",
       " 'faze',\n",
       " 'discriminating',\n",
       " 'slovenian',\n",
       " 'inconsequential',\n",
       " 'ghskepticalannoyedundecideduneasyorhesitanth',\n",
       " '397',\n",
       " 'muwahahaha',\n",
       " 'canot',\n",
       " 'poisson',\n",
       " 'gca',\n",
       " 'blizz',\n",
       " 'walkway',\n",
       " 'bbycks',\n",
       " 'awll',\n",
       " 'pacemaker',\n",
       " 'strickland',\n",
       " 'skied',\n",
       " 'suena',\n",
       " 'unstraightened',\n",
       " 'stevenson',\n",
       " 'hoyt',\n",
       " 'appleby',\n",
       " 'caitlins',\n",
       " 'biten',\n",
       " 'bestiie',\n",
       " 'barrera',\n",
       " 'existentialism',\n",
       " 'btwww',\n",
       " 'westjet',\n",
       " 'helpme',\n",
       " '1o',\n",
       " 'ploughing',\n",
       " 'nighht',\n",
       " 'sitc',\n",
       " 'gudmornin',\n",
       " 'gpo',\n",
       " 'pleeze',\n",
       " 'fishsticks',\n",
       " 'zinging',\n",
       " 'kenzo',\n",
       " 'annemarie',\n",
       " 'hugeeee',\n",
       " 'minerva',\n",
       " 'asd',\n",
       " 'lotioned',\n",
       " 'mahilig',\n",
       " 'mugglespace',\n",
       " 'bitmap',\n",
       " 'shoestring',\n",
       " 'paperrr',\n",
       " 'gymmmm',\n",
       " 'jethro',\n",
       " 'busdriver',\n",
       " 'visi',\n",
       " 'desrve',\n",
       " 'bitchhh',\n",
       " 'penyet',\n",
       " 'qaeda',\n",
       " 'coms',\n",
       " 'bootylicious',\n",
       " 'ndreams',\n",
       " 'copyin',\n",
       " 'truest',\n",
       " 'unmentionable',\n",
       " 'woooohoooooo',\n",
       " 'carolyne',\n",
       " 'wsb',\n",
       " 'panelist',\n",
       " 'penpal',\n",
       " '1910',\n",
       " 'carcinogen',\n",
       " 'ntsc',\n",
       " 'spanning',\n",
       " 'ignacio',\n",
       " 'nottttttt',\n",
       " 'pomegrante',\n",
       " 'ascared',\n",
       " 'freakinn',\n",
       " 'acetone',\n",
       " 'mby',\n",
       " 'yoour',\n",
       " 'baaaaaaaaaaaad',\n",
       " 'vuze',\n",
       " 'situ',\n",
       " 'kubica',\n",
       " 'bummm',\n",
       " 'mined',\n",
       " 'chela',\n",
       " 'forman',\n",
       " 'seaton',\n",
       " 'adl',\n",
       " 'absoulutley',\n",
       " 'lovett',\n",
       " 'danskin',\n",
       " 'radian6',\n",
       " '3cm',\n",
       " 'excusable',\n",
       " 'delgado',\n",
       " 'amke',\n",
       " 'spammage',\n",
       " 'maimed',\n",
       " 'babz',\n",
       " 'wahou',\n",
       " 'redecorate',\n",
       " 'coreys',\n",
       " 'bigmac',\n",
       " 'aparece',\n",
       " 'chanels',\n",
       " 'hottop',\n",
       " 'chatbox',\n",
       " 'ê³¼',\n",
       " 'ixus',\n",
       " 'hhahha',\n",
       " 'jackrabbit',\n",
       " 'nyoba',\n",
       " 'quynh',\n",
       " 'upsetted',\n",
       " 'jalil',\n",
       " 'myyyyy',\n",
       " 'storytime',\n",
       " 'boredddddddddddd',\n",
       " 'girlfrend',\n",
       " 'binondo',\n",
       " 'lyra',\n",
       " 'schultz',\n",
       " 'huuuu',\n",
       " 'metrocard',\n",
       " '2happyfacesmiley5am',\n",
       " 'sunhine',\n",
       " 'leid',\n",
       " 'fazeley',\n",
       " 'introverted',\n",
       " 'derol',\n",
       " 'snugged',\n",
       " 'bantay',\n",
       " 'bentonville',\n",
       " 'aiea',\n",
       " 'heterosexual',\n",
       " 'screenwriting',\n",
       " 'humus',\n",
       " 'ween',\n",
       " 'tgthr',\n",
       " 'princesammie',\n",
       " 'indomie',\n",
       " 'costo',\n",
       " 'ambil',\n",
       " 'manan',\n",
       " 'donator',\n",
       " 'priestess',\n",
       " 'oooooohhh',\n",
       " 'penat',\n",
       " 'ayeeeee',\n",
       " 'unfortuately',\n",
       " 'radley',\n",
       " 'pinkk',\n",
       " 'wolfcat',\n",
       " 'tengok',\n",
       " 'chnage',\n",
       " 'plesk',\n",
       " 'trickery',\n",
       " 'dammmm',\n",
       " 'lyall',\n",
       " '10years',\n",
       " 'oxidation',\n",
       " 'vcd',\n",
       " 'gayi',\n",
       " 'rinsing',\n",
       " 'kng',\n",
       " 'hydra',\n",
       " 'xmarks',\n",
       " 'ggrrrrr',\n",
       " 'jalisco',\n",
       " 'twooooo',\n",
       " 'betterrrrr',\n",
       " 'retrain',\n",
       " 'merciless',\n",
       " 'hourz',\n",
       " 'cometh',\n",
       " 'angrey',\n",
       " '³é',\n",
       " 'unisom',\n",
       " 'stuying',\n",
       " 'canai',\n",
       " 'puyallup',\n",
       " 'fucck',\n",
       " 'zhen',\n",
       " 'lovelyness',\n",
       " 'nothins',\n",
       " 'looovvveee',\n",
       " '11hr',\n",
       " 'thta',\n",
       " 'hamel',\n",
       " 'kab',\n",
       " 'etcetc',\n",
       " 'paci',\n",
       " 'clowdy',\n",
       " 'oranje',\n",
       " 'obgyn',\n",
       " 'perlman',\n",
       " 'snowboarder',\n",
       " 'dissect',\n",
       " 'shyne',\n",
       " '3l',\n",
       " 'scrutiny',\n",
       " 'tuy',\n",
       " 'tiago',\n",
       " 'determind',\n",
       " 'llo',\n",
       " 'timeshare',\n",
       " 'weakening',\n",
       " 'jamma',\n",
       " 'chiling',\n",
       " 'ceritanya',\n",
       " '400g',\n",
       " 'adree',\n",
       " 'conando',\n",
       " 'txtt',\n",
       " '30000',\n",
       " 'picnicing',\n",
       " 'whers',\n",
       " 'heightened',\n",
       " 'verdi',\n",
       " 'caa',\n",
       " 'belives',\n",
       " 'kiitos',\n",
       " 'cervix',\n",
       " '902',\n",
       " 'vergeten',\n",
       " 'jacobo',\n",
       " 'khristina',\n",
       " 'huuungry',\n",
       " 'osurprisedoo',\n",
       " '984',\n",
       " '643',\n",
       " '357',\n",
       " 'd6',\n",
       " 'd12',\n",
       " 'bsy',\n",
       " 'd20',\n",
       " 'pracs',\n",
       " 'brittanya',\n",
       " 'tireeeeeeeeed',\n",
       " 'hammies',\n",
       " 'jacon',\n",
       " 'almere',\n",
       " 'nast',\n",
       " 'yewwwww',\n",
       " 'escargot',\n",
       " 'freeeeeeezing',\n",
       " 'sophi',\n",
       " 'tutored',\n",
       " 'goodmorningg',\n",
       " 'soooory',\n",
       " 'shemah',\n",
       " 'homebased',\n",
       " 'blck',\n",
       " 'normalcy',\n",
       " 'motorhome',\n",
       " 'umpa',\n",
       " 'mstrkrft',\n",
       " '15mph',\n",
       " 'fumble',\n",
       " 'illll',\n",
       " 'geekyness',\n",
       " 'biro',\n",
       " 'tadaaa',\n",
       " 'amout',\n",
       " 'nighttttttt',\n",
       " 'woothemes',\n",
       " 'saimin',\n",
       " 'zm',\n",
       " 'danks',\n",
       " 'deron',\n",
       " 'justnow',\n",
       " 'nampa',\n",
       " 'fpr',\n",
       " 'mitzy',\n",
       " 'nedã',\n",
       " 'ayn',\n",
       " 'pannels',\n",
       " 'x100000',\n",
       " 'bernardo',\n",
       " 'lebrons',\n",
       " 'escalating',\n",
       " 'ramz',\n",
       " 'jarring',\n",
       " 'knebworth',\n",
       " 'kneed',\n",
       " 'marrakech',\n",
       " 'cudve',\n",
       " 'gennaro',\n",
       " '5l',\n",
       " 'anthrax',\n",
       " 'mmmbop',\n",
       " 'isync',\n",
       " 'texmex',\n",
       " 'haaaha',\n",
       " 'beboing',\n",
       " 'ghãª',\n",
       " 'coathangers',\n",
       " 'oioi',\n",
       " 'ªè',\n",
       " 'differant',\n",
       " 'typist',\n",
       " 'airlifted',\n",
       " 'photshoot',\n",
       " 'yappin',\n",
       " 'callender',\n",
       " 'hungryyyyyyy',\n",
       " 'tse',\n",
       " 'pleaure',\n",
       " 'estatic',\n",
       " 'wvu',\n",
       " 'devizes',\n",
       " 'neils',\n",
       " 'wc3',\n",
       " 'bbwl',\n",
       " 'ggg',\n",
       " 'taekwon',\n",
       " 'mle',\n",
       " 'goodwood',\n",
       " 'imso',\n",
       " 'klu',\n",
       " 'loach',\n",
       " 'compat',\n",
       " 'wooooork',\n",
       " 'wilding',\n",
       " 'easyway',\n",
       " '774',\n",
       " 'forgots',\n",
       " 'collapsable',\n",
       " 'twinz',\n",
       " 'donnell',\n",
       " 'scotlandd',\n",
       " 'hangnail',\n",
       " 'feelingg',\n",
       " 'diagonal',\n",
       " 'docomo',\n",
       " 'e65',\n",
       " 'rimini',\n",
       " 'fbk',\n",
       " 'topstyle',\n",
       " 'shazza',\n",
       " 'euuugh',\n",
       " 'zombieville',\n",
       " 'polisci',\n",
       " 'befriended',\n",
       " 'margaux',\n",
       " 'naaaap',\n",
       " 'boredomm',\n",
       " 'stii',\n",
       " 'vommed',\n",
       " 'teck',\n",
       " 'bonafide',\n",
       " 'evill',\n",
       " 'cism',\n",
       " 'hotting',\n",
       " 'mvie',\n",
       " 'ergg',\n",
       " 'ascend',\n",
       " 'cep',\n",
       " 'bkgrd',\n",
       " '57am',\n",
       " 'partents',\n",
       " 'quigg',\n",
       " 'waaaaayyyyy',\n",
       " 'norse',\n",
       " 'intra',\n",
       " 'lmfaooooooo',\n",
       " 'ogts',\n",
       " 'rewound',\n",
       " 'e30',\n",
       " 'musk',\n",
       " 'teefs',\n",
       " 'muneca',\n",
       " '½te',\n",
       " 'ferriss',\n",
       " 'gyp',\n",
       " 'sheeeet',\n",
       " 'spritz',\n",
       " 'stimmt',\n",
       " 'ineptitude',\n",
       " 'tennants',\n",
       " 'unforseen',\n",
       " 'idate',\n",
       " 'buta',\n",
       " 'goodsex',\n",
       " 'achar',\n",
       " 'springbok',\n",
       " 'kyan',\n",
       " 'mandela',\n",
       " 'nickcarter',\n",
       " 'panara',\n",
       " 'minder',\n",
       " 'anooping',\n",
       " 'telle',\n",
       " 'congratulations',\n",
       " 'damier',\n",
       " '860',\n",
       " 'stupidddd',\n",
       " 'beutifull',\n",
       " 'therfore',\n",
       " 'easyjet',\n",
       " 'procter',\n",
       " 'aaaaaaages',\n",
       " 'matcha',\n",
       " 'chewable',\n",
       " 'eastlink',\n",
       " 'intown',\n",
       " 'tora',\n",
       " 'hider',\n",
       " 'immunisation',\n",
       " 'gaggin',\n",
       " 'noras',\n",
       " 'harkins',\n",
       " 'hoarding',\n",
       " '¹æ',\n",
       " 'kristins',\n",
       " 'bucketful',\n",
       " 'morsel',\n",
       " 'whiile',\n",
       " 'airmail',\n",
       " 'sameeee',\n",
       " 'mbta',\n",
       " 'abcess',\n",
       " 'evning',\n",
       " 'whoooole',\n",
       " 'followw',\n",
       " 'cleverly',\n",
       " 'restock',\n",
       " 'holis',\n",
       " 'gibi',\n",
       " 'vondelpark',\n",
       " 'nokias',\n",
       " 'whaaaaaaaaaaat',\n",
       " 'sanction',\n",
       " '933',\n",
       " 'nightowl',\n",
       " 'propeller',\n",
       " 'kelar',\n",
       " 'daquiri',\n",
       " 'dc3',\n",
       " 'siccck',\n",
       " 'inquirer',\n",
       " 'rees',\n",
       " 'fuunnn',\n",
       " 'gret',\n",
       " 'vemos',\n",
       " 'opposable',\n",
       " 'coookies',\n",
       " 'lesbos',\n",
       " 'jobcentre',\n",
       " '4all',\n",
       " 'jobb',\n",
       " 'drinktank',\n",
       " 'qype',\n",
       " 'cmi',\n",
       " 'impacting',\n",
       " 'cuzs',\n",
       " 'kettlecorn',\n",
       " 'wrd',\n",
       " 'slushee',\n",
       " 'angell',\n",
       " 'nooooot',\n",
       " '18pm',\n",
       " 'wokr',\n",
       " 'excitetonguestickingoutcheekyplayfulorblowingaraspberry',\n",
       " 'arrowhead',\n",
       " 'smodcast',\n",
       " '³à¹',\n",
       " '237',\n",
       " 'depraved',\n",
       " 'debuting',\n",
       " 'aboohoo',\n",
       " 'ts2',\n",
       " 'radioshow',\n",
       " 'jellytots',\n",
       " 'singled',\n",
       " 'piriteze',\n",
       " 'propensity',\n",
       " 'cheaaa',\n",
       " 'yajl',\n",
       " 'nikky',\n",
       " 'meddling',\n",
       " 'smashd',\n",
       " 'boolean',\n",
       " 'yuuummm',\n",
       " 'trico',\n",
       " 'danimals',\n",
       " 'noronha',\n",
       " 'husker',\n",
       " 'enforcing',\n",
       " 'provecho',\n",
       " 'erudite',\n",
       " 'morten',\n",
       " 'pesach',\n",
       " 'appy',\n",
       " 'sinon',\n",
       " 'huggeee',\n",
       " 'joshi',\n",
       " 'awwwwwwwwwwwwwwww',\n",
       " 'incriminating',\n",
       " 'stupied',\n",
       " 'chente',\n",
       " 'borinq',\n",
       " 'cavalry',\n",
       " 'empy',\n",
       " 'photies',\n",
       " 'adorei',\n",
       " 'incubation',\n",
       " 'soldiering',\n",
       " 'poå',\n",
       " 'wagga',\n",
       " 'nyack',\n",
       " 'heckler',\n",
       " 'canx',\n",
       " 'ampwinkorsmirkad',\n",
       " 'dã¹',\n",
       " 'pollyanna',\n",
       " 'primate',\n",
       " 'cultus',\n",
       " 'ká',\n",
       " 'climatic',\n",
       " 'btt',\n",
       " 'ktt',\n",
       " 'kinnda',\n",
       " 'chilax',\n",
       " 'adgi',\n",
       " 'assesments',\n",
       " 'overextended',\n",
       " 'biggs',\n",
       " 'hilt',\n",
       " 'mamy',\n",
       " 'srv',\n",
       " 'rapunzel',\n",
       " 'theyl',\n",
       " 'doughy',\n",
       " 'yã',\n",
       " 'isolde',\n",
       " 'psl',\n",
       " 'mfh',\n",
       " 'nutragrain',\n",
       " 'upperclassmen',\n",
       " 'astrologyzone',\n",
       " 'lalalaaa',\n",
       " 'chrystal',\n",
       " 'carlson',\n",
       " 'flanigan',\n",
       " 'pumkin',\n",
       " 'snowdonia',\n",
       " 'karli',\n",
       " 'delts',\n",
       " 'scrounging',\n",
       " 'wooded',\n",
       " 'chickenburger',\n",
       " 'chicfila',\n",
       " 'jumpcut',\n",
       " 'shuttup',\n",
       " 'aes',\n",
       " 'iwork',\n",
       " 'ilford',\n",
       " 'guages',\n",
       " 'dmi',\n",
       " 'valdez',\n",
       " 'suppper',\n",
       " '24k',\n",
       " 'mildy',\n",
       " 'handpainted',\n",
       " 'membuat',\n",
       " 'parentless',\n",
       " 'wone',\n",
       " 'bekka',\n",
       " '6wks',\n",
       " 'ryerson',\n",
       " 'bdc',\n",
       " '3mos',\n",
       " 'updateee',\n",
       " 'phillippines',\n",
       " 'sorer',\n",
       " 'demerit',\n",
       " 'iãªn',\n",
       " 'lamingtons',\n",
       " 'stiffed',\n",
       " 'ryde',\n",
       " 'schoolbus',\n",
       " 'guesser',\n",
       " 'nungguin',\n",
       " 'realii',\n",
       " 'leaved',\n",
       " 'imposing',\n",
       " 'nagy',\n",
       " 'tashas',\n",
       " 'warranted',\n",
       " 'sambil',\n",
       " 'pumba',\n",
       " 'golek',\n",
       " 'souk',\n",
       " 'moab',\n",
       " 'cheaps',\n",
       " 'snuffing',\n",
       " 'lortab',\n",
       " 'transporting',\n",
       " 'oohhhhhh',\n",
       " 'baaaaddd',\n",
       " 'gotomeeting',\n",
       " 'matchstick',\n",
       " 'jamas',\n",
       " 'twittertown',\n",
       " 'boreedddd',\n",
       " 'caicos',\n",
       " 'habra',\n",
       " 'monopolize',\n",
       " 'russert',\n",
       " 'nonfunctional',\n",
       " 'douchbags',\n",
       " 'knowlege',\n",
       " 'andito',\n",
       " 'doodily',\n",
       " 'dammitt',\n",
       " 'jcc',\n",
       " 'hustled',\n",
       " 'mundays',\n",
       " 'mahna',\n",
       " 'jeremie',\n",
       " 'plizzz',\n",
       " 'wpa',\n",
       " 'buah',\n",
       " 'unsick',\n",
       " 'garin',\n",
       " 'nothiing',\n",
       " 'toniiiight',\n",
       " 'manee',\n",
       " 'koto',\n",
       " 'gradutation',\n",
       " 'hoomee',\n",
       " 'owhhh',\n",
       " 'signer',\n",
       " 'supposes',\n",
       " 'trannys',\n",
       " 'assisstant',\n",
       " 'obtaining',\n",
       " 'contraire',\n",
       " 'dependant',\n",
       " 'lazzara',\n",
       " 'isetan',\n",
       " 'tryign',\n",
       " 'scandelous',\n",
       " 'bronagh',\n",
       " 'mcing',\n",
       " 'c0uld',\n",
       " 'hmmmmph',\n",
       " 'informer',\n",
       " '361',\n",
       " 'muskateers',\n",
       " 'kaga',\n",
       " 'interception',\n",
       " 'incher',\n",
       " 'hellokitty',\n",
       " 'surpirse',\n",
       " 'thouuu',\n",
       " 'beeotch',\n",
       " 'disintegrated',\n",
       " 'backe',\n",
       " 'toally',\n",
       " 'thsi',\n",
       " 'rumblin',\n",
       " 'offed',\n",
       " 'brobee',\n",
       " 'postpaid',\n",
       " 'gumboots',\n",
       " 'pasteis',\n",
       " 'lisaaa',\n",
       " 'gow3',\n",
       " 'snce',\n",
       " 'infallible',\n",
       " 'kratos',\n",
       " 'signalling',\n",
       " 'thundered',\n",
       " '401k',\n",
       " 'likt',\n",
       " 'tomorrowwwwww',\n",
       " 'authenticity',\n",
       " 'undercooked',\n",
       " 'lovvveee',\n",
       " 'osborn',\n",
       " 'sittinq',\n",
       " 'nauseating',\n",
       " 'crimewatch',\n",
       " 'dte',\n",
       " 'skwel',\n",
       " 'baseband',\n",
       " 'aaarrgghh',\n",
       " 'lineage',\n",
       " '683',\n",
       " 'payn',\n",
       " 'ialso',\n",
       " 'winans',\n",
       " 'mfa',\n",
       " 'biscayne',\n",
       " 'twitworld',\n",
       " 'styleeee',\n",
       " 'wuii',\n",
       " 'oritse',\n",
       " 'agaaaaain',\n",
       " '407',\n",
       " 'heartening',\n",
       " '718',\n",
       " 'forgetfulness',\n",
       " 'happed',\n",
       " '²ã',\n",
       " '97s',\n",
       " 'deleated',\n",
       " 'oswestry',\n",
       " 'hyunjoong',\n",
       " 'lecter',\n",
       " 'boathouse',\n",
       " 'lod',\n",
       " 'cinny',\n",
       " 'hillside',\n",
       " 'slicehost',\n",
       " 'gherkin',\n",
       " 'awaaaaay',\n",
       " 'mueseum',\n",
       " 'sansa',\n",
       " 'bonifacio',\n",
       " 'kendrick',\n",
       " 'achou',\n",
       " 'tanz',\n",
       " 'halloooo',\n",
       " 'sometin',\n",
       " 'leilani',\n",
       " 'aclu',\n",
       " 'weiners',\n",
       " 'jovan',\n",
       " 'beaty',\n",
       " 'survivalism',\n",
       " 'newsom',\n",
       " 'eveer',\n",
       " 'kdramas',\n",
       " 'zhu',\n",
       " 'rev3',\n",
       " 'catskills',\n",
       " 'lauging',\n",
       " 'gahaha',\n",
       " 'natalee',\n",
       " '1938',\n",
       " 'crier',\n",
       " 'neef',\n",
       " 'splotch',\n",
       " 'olc',\n",
       " 'showeer',\n",
       " 'dlna',\n",
       " 'baaarf',\n",
       " 'pleaded',\n",
       " 'boiii',\n",
       " 'bedrock',\n",
       " 'bbay',\n",
       " 'carlise',\n",
       " 'mroning',\n",
       " 'bastion',\n",
       " 'henyo',\n",
       " 'twiggers',\n",
       " 'yayyyyyyyyyyyy',\n",
       " 'elegantly',\n",
       " 'noonan',\n",
       " 'consonant',\n",
       " 'fritzl',\n",
       " 'magichat',\n",
       " 'zombified',\n",
       " 'fomo',\n",
       " 'fightn',\n",
       " 'uhul',\n",
       " 'authorization',\n",
       " 'rottaboat',\n",
       " 'cafeine',\n",
       " 'mwuahaha',\n",
       " 'astaire',\n",
       " 'coho',\n",
       " 'elway',\n",
       " 'cotter',\n",
       " 'seawall',\n",
       " 'pieter',\n",
       " 'scootering',\n",
       " 'franns',\n",
       " 'moreish',\n",
       " 'cornrows',\n",
       " 'nnnn',\n",
       " 'hemm',\n",
       " 'cuatro',\n",
       " 'terminating',\n",
       " 'polygamy',\n",
       " 'shahi',\n",
       " 'stroganoff',\n",
       " 'miya',\n",
       " 'typo3',\n",
       " '998',\n",
       " 'leveraging',\n",
       " 'mondayyyy',\n",
       " '2follow',\n",
       " 'cackle',\n",
       " 'nawwws',\n",
       " 'jibber',\n",
       " 'tcg',\n",
       " 'likke',\n",
       " 'mating',\n",
       " 'mancunian',\n",
       " 'dorama',\n",
       " 'nefew',\n",
       " 'blaahhh',\n",
       " 'tehn',\n",
       " 'benci',\n",
       " 'powerlines',\n",
       " 'drivee',\n",
       " 'emitting',\n",
       " 'taff',\n",
       " 'endometriosis',\n",
       " 'throath',\n",
       " 'quotwinkorsmirkesperate',\n",
       " 'busily',\n",
       " 'notifying',\n",
       " 'dã¼sseldorf',\n",
       " 'bowleg',\n",
       " 'aaarrgh',\n",
       " 'sleepâ',\n",
       " 'confuseddd',\n",
       " 'paket',\n",
       " 'zam',\n",
       " 'dissected',\n",
       " 'counterproductive',\n",
       " 'geof',\n",
       " 'brahh',\n",
       " 'slant',\n",
       " 'shelbs',\n",
       " 'ladyboys',\n",
       " 'maka',\n",
       " 'keema',\n",
       " 'administer',\n",
       " 'locos',\n",
       " 'bookbuzzr',\n",
       " 'putih',\n",
       " 'tagless',\n",
       " 'dabba',\n",
       " 'superduperhugs',\n",
       " 'madworld',\n",
       " 'thinkz',\n",
       " 'planetshakers',\n",
       " 'hookey',\n",
       " 'simei',\n",
       " 'madela',\n",
       " 'tickly',\n",
       " 'baroque',\n",
       " 'freesat',\n",
       " 'sembreak',\n",
       " 'activ',\n",
       " 'fraternity',\n",
       " 'seriouse',\n",
       " 'ð¾ð³ð¾ð',\n",
       " 'kingsgate',\n",
       " 'fllw',\n",
       " 'mmrs',\n",
       " 'limon',\n",
       " 'airasia',\n",
       " 'operandi',\n",
       " 'jochen',\n",
       " 'tutup',\n",
       " 'candyfloss',\n",
       " 'officailly',\n",
       " 'maclaren',\n",
       " 'twp',\n",
       " 'duffle',\n",
       " 'northrend',\n",
       " 'mwahah',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_split_list = words_to_split['word'].tolist()\n",
    "words_to_split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_use_words = set(words_to_split_list) # Convert your list to a set for faster lookup\n",
    "\n",
    "def remove_single_use_words(tweet):\n",
    "    return [word for word in tweet if word not in single_use_words]\n",
    "\n",
    "data['tweet_tokenized'] = data['tweet_tokenized'].apply(remove_single_use_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweet_tokenized_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[, awww, bummer, shoulda, got, david, carr, th...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>awww bummer shoulda got david carr third day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, update, facebook, texting, might, cry,...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[many, time, ball, managed, save, 50, rest, go...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>many time ball managed save 50 rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, mad, see, ]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>[com, cool, hear, old, walt, interview, â, ]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>com cool hear old walt interview â</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>[ready, mojo, makeover, ask, detail]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>[happy, 38th, birthday, boo, alll, time, tupac...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happy 38th birthday boo alll time tupac shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:50</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id             user  \\\n",
       "0                0  1467810369  _TheSpecialOne_   \n",
       "1                0  1467810672    scotthamilton   \n",
       "2                0  1467810917         mattycus   \n",
       "3                0  1467811184          ElleCTF   \n",
       "4                0  1467811193           Karoli   \n",
       "...            ...         ...              ...   \n",
       "1599995          1  2193601966  AmandaMarie1028   \n",
       "1599996          1  2193601969      TheWDBoards   \n",
       "1599997          1  2193601991           bpbabe   \n",
       "1599998          1  2193602064     tinydiamondz   \n",
       "1599999          1  2193602129   RyanTrevMorris   \n",
       "\n",
       "                                                     tweet  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1        is upset that he can't update his Facebook by ...   \n",
       "2        @Kenichan I dived many times for the ball. Man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1599995  Just woke up. Having no school is the best fee...   \n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                           tweet_tokenized  \\\n",
       "0        [, awww, bummer, shoulda, got, david, carr, th...   \n",
       "1        [upset, update, facebook, texting, might, cry,...   \n",
       "2        [many, time, ball, managed, save, 50, rest, go...   \n",
       "3                   [whole, body, feel, itchy, like, fire]   \n",
       "4                                   [behaving, mad, see, ]   \n",
       "...                                                    ...   \n",
       "1599995                [woke, school, best, feeling, ever]   \n",
       "1599996       [com, cool, hear, old, walt, interview, â, ]   \n",
       "1599997               [ready, mojo, makeover, ask, detail]   \n",
       "1599998  [happy, 38th, birthday, boo, alll, time, tupac...   \n",
       "1599999                                            [happy]   \n",
       "\n",
       "                       date  year  month  weekday      time  ...  word_count  \\\n",
       "0       2009-04-06 22:19:45  2009      4        0  22:19:45  ...          19   \n",
       "1       2009-04-06 22:19:49  2009      4        0  22:19:49  ...          21   \n",
       "2       2009-04-06 22:19:53  2009      4        0  22:19:53  ...          18   \n",
       "3       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          10   \n",
       "4       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          21   \n",
       "...                     ...   ...    ...      ...       ...  ...         ...   \n",
       "1599995 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599996 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599997 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1599998 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          12   \n",
       "1599999 2009-06-16 08:40:50  2009      6        1  08:40:50  ...           5   \n",
       "\n",
       "        dot_dot_dot  exclamation_mark  question_mark  at_symbol  link  money  \\\n",
       "0                 0                 0              0          1     1      0   \n",
       "1                 1                 1              0          0     0      0   \n",
       "2                 0                 0              0          1     0      0   \n",
       "3                 0                 0              0          0     0      0   \n",
       "4                 0                 0              1          1     0      0   \n",
       "...             ...               ...            ...        ...   ...    ...   \n",
       "1599995           0                 0              0          0     0      0   \n",
       "1599996           0                 1              0          0     1      0   \n",
       "1599997           0                 0              1          0     0      0   \n",
       "1599998           0                 1              0          0     0      0   \n",
       "1599999           0                 0              0          1     0      0   \n",
       "\n",
       "         paragraph_symbol  hashtag  \\\n",
       "0                       0        0   \n",
       "1                       0        0   \n",
       "2                       0        0   \n",
       "3                       0        0   \n",
       "4                       0        0   \n",
       "...                   ...      ...   \n",
       "1599995                 0        0   \n",
       "1599996                 0        0   \n",
       "1599997                 0        0   \n",
       "1599998                 0        0   \n",
       "1599999                 0        1   \n",
       "\n",
       "                                    tweet_tokenized_string  \n",
       "0         awww bummer shoulda got david carr third day ...  \n",
       "1        upset update facebook texting might cry result...  \n",
       "2             many time ball managed save 50 rest go bound  \n",
       "3                          whole body feel itchy like fire  \n",
       "4                                        behaving mad see   \n",
       "...                                                    ...  \n",
       "1599995                      woke school best feeling ever  \n",
       "1599996                com cool hear old walt interview â   \n",
       "1599997                     ready mojo makeover ask detail  \n",
       "1599998     happy 38th birthday boo alll time tupac shakur  \n",
       "1599999                                              happy  \n",
       "\n",
       "[1600000 rows x 22 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N-gram vectorizing and tfidf need a list of strings passed to it, so we need to convert our list\n",
    "data['tweet_tokenized_string'] = data['tweet_tokenized'].apply(lambda x: ' '.join(x)) #Join each word in our list with a space inbetween\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wenn wir wollen, können wir diese Tweets alle herausfiltern (Dafür einfach das False in True umwandeln)\n",
    "data = data[data['tweet_tokenized_string'].str.contains('[a-zA-Z]')==True] # Alle Tweet herausfiltern die nur nicht normale Buchstaben enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>weekday</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count</th>\n",
       "      <th>dot_dot_dot</th>\n",
       "      <th>exclamation_mark</th>\n",
       "      <th>question_mark</th>\n",
       "      <th>at_symbol</th>\n",
       "      <th>link</th>\n",
       "      <th>money</th>\n",
       "      <th>paragraph_symbol</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>tweet_tokenized_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[, awww, bummer, shoulda, got, david, carr, th...</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:45</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>awww bummer shoulda got david carr third day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, update, facebook, texting, might, cry,...</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:49</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[many, time, ball, managed, save, 50, rest, go...</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:53</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>many time ball managed save 50 rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, body, feel, itchy, like, fire]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[behaving, mad, see, ]</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22:19:57</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588280</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588281</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>[com, cool, hear, old, walt, interview, â, ]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>com cool hear old walt interview â</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588282</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>[ready, mojo, makeover, ask, detail]</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588283</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>[happy, 38th, birthday, boo, alll, time, tupac...</td>\n",
       "      <td>2009-06-16 08:40:49</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:49</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happy 38th birthday boo alll time tupac shakur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588284</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>[happy]</td>\n",
       "      <td>2009-06-16 08:40:50</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>08:40:50</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1588285 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id             user  \\\n",
       "0                0  1467810369  _TheSpecialOne_   \n",
       "1                0  1467810672    scotthamilton   \n",
       "2                0  1467810917         mattycus   \n",
       "3                0  1467811184          ElleCTF   \n",
       "4                0  1467811193           Karoli   \n",
       "...            ...         ...              ...   \n",
       "1588280          1  2193601966  AmandaMarie1028   \n",
       "1588281          1  2193601969      TheWDBoards   \n",
       "1588282          1  2193601991           bpbabe   \n",
       "1588283          1  2193602064     tinydiamondz   \n",
       "1588284          1  2193602129   RyanTrevMorris   \n",
       "\n",
       "                                                     tweet  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1        is upset that he can't update his Facebook by ...   \n",
       "2        @Kenichan I dived many times for the ball. Man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1588280  Just woke up. Having no school is the best fee...   \n",
       "1588281  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1588282  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1588283  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1588284  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                           tweet_tokenized  \\\n",
       "0        [, awww, bummer, shoulda, got, david, carr, th...   \n",
       "1        [upset, update, facebook, texting, might, cry,...   \n",
       "2        [many, time, ball, managed, save, 50, rest, go...   \n",
       "3                   [whole, body, feel, itchy, like, fire]   \n",
       "4                                   [behaving, mad, see, ]   \n",
       "...                                                    ...   \n",
       "1588280                [woke, school, best, feeling, ever]   \n",
       "1588281       [com, cool, hear, old, walt, interview, â, ]   \n",
       "1588282               [ready, mojo, makeover, ask, detail]   \n",
       "1588283  [happy, 38th, birthday, boo, alll, time, tupac...   \n",
       "1588284                                            [happy]   \n",
       "\n",
       "                       date  year  month  weekday      time  ...  word_count  \\\n",
       "0       2009-04-06 22:19:45  2009      4        0  22:19:45  ...          19   \n",
       "1       2009-04-06 22:19:49  2009      4        0  22:19:49  ...          21   \n",
       "2       2009-04-06 22:19:53  2009      4        0  22:19:53  ...          18   \n",
       "3       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          10   \n",
       "4       2009-04-06 22:19:57  2009      4        0  22:19:57  ...          21   \n",
       "...                     ...   ...    ...      ...       ...  ...         ...   \n",
       "1588280 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1588281 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1588282 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          11   \n",
       "1588283 2009-06-16 08:40:49  2009      6        1  08:40:49  ...          12   \n",
       "1588284 2009-06-16 08:40:50  2009      6        1  08:40:50  ...           5   \n",
       "\n",
       "        dot_dot_dot  exclamation_mark  question_mark  at_symbol  link  money  \\\n",
       "0                 0                 0              0          1     1      0   \n",
       "1                 1                 1              0          0     0      0   \n",
       "2                 0                 0              0          1     0      0   \n",
       "3                 0                 0              0          0     0      0   \n",
       "4                 0                 0              1          1     0      0   \n",
       "...             ...               ...            ...        ...   ...    ...   \n",
       "1588280           0                 0              0          0     0      0   \n",
       "1588281           0                 1              0          0     1      0   \n",
       "1588282           0                 0              1          0     0      0   \n",
       "1588283           0                 1              0          0     0      0   \n",
       "1588284           0                 0              0          1     0      0   \n",
       "\n",
       "         paragraph_symbol  hashtag  \\\n",
       "0                       0        0   \n",
       "1                       0        0   \n",
       "2                       0        0   \n",
       "3                       0        0   \n",
       "4                       0        0   \n",
       "...                   ...      ...   \n",
       "1588280                 0        0   \n",
       "1588281                 0        0   \n",
       "1588282                 0        0   \n",
       "1588283                 0        0   \n",
       "1588284                 0        1   \n",
       "\n",
       "                                    tweet_tokenized_string  \n",
       "0         awww bummer shoulda got david carr third day ...  \n",
       "1        upset update facebook texting might cry result...  \n",
       "2             many time ball managed save 50 rest go bound  \n",
       "3                          whole body feel itchy like fire  \n",
       "4                                        behaving mad see   \n",
       "...                                                    ...  \n",
       "1588280                      woke school best feeling ever  \n",
       "1588281                com cool hear old walt interview â   \n",
       "1588282                     ready mojo makeover ask detail  \n",
       "1588283     happy 38th birthday boo alll time tupac shakur  \n",
       "1588284                                              happy  \n",
       "\n",
       "[1588285 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index löschen um ihn neu zu nummerieren da sich die Anzahl der Tweets geändert hat\n",
    "data = data.reset_index()\n",
    "data = data.drop(columns=['index'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1588285 entries, 0 to 1588284\n",
      "Data columns (total 22 columns):\n",
      " #   Column                  Non-Null Count    Dtype         \n",
      "---  ------                  --------------    -----         \n",
      " 0   sentiment               1588285 non-null  int64         \n",
      " 1   id                      1588285 non-null  int64         \n",
      " 2   user                    1588285 non-null  object        \n",
      " 3   tweet                   1588285 non-null  object        \n",
      " 4   tweet_tokenized         1588285 non-null  object        \n",
      " 5   date                    1588285 non-null  datetime64[ns]\n",
      " 6   year                    1588285 non-null  int32         \n",
      " 7   month                   1588285 non-null  int32         \n",
      " 8   weekday                 1588285 non-null  int32         \n",
      " 9   time                    1588285 non-null  object        \n",
      " 10  hour                    1588285 non-null  int32         \n",
      " 11  time_group              1588285 non-null  object        \n",
      " 12  word_count              1588285 non-null  int64         \n",
      " 13  dot_dot_dot             1588285 non-null  int64         \n",
      " 14  exclamation_mark        1588285 non-null  int64         \n",
      " 15  question_mark           1588285 non-null  int64         \n",
      " 16  at_symbol               1588285 non-null  int64         \n",
      " 17  link                    1588285 non-null  int64         \n",
      " 18  money                   1588285 non-null  int64         \n",
      " 19  paragraph_symbol        1588285 non-null  int64         \n",
      " 20  hashtag                 1588285 non-null  int64         \n",
      " 21  tweet_tokenized_string  1588285 non-null  object        \n",
      "dtypes: datetime64[ns](1), int32(4), int64(11), object(6)\n",
      "memory usage: 242.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['id', 'user', 'tweet', 'tweet_tokenized', 'date', 'year', 'month', 'time', 'time_group'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" data.to_pickle('data_cleaned') \""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" data.to_pickle('data_cleaned') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data in Dependent and indepentent variable\n",
    "X = data.drop(columns=['sentiment'], axis = 1)\n",
    "y= data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.2-cp39-cp39-win_amd64.whl (8.4 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.2.2 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split into 3 sets (Training, Validation and Test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation_and_test, y_train, y_validation_and_test = train_test_split(X, y, test_size=.1, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_validation_and_test, y_validation_and_test, test_size=.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1429456 entries with 50.05% negative, 49.95% positive\n",
      "Validation set has total 142946 entries with 50.07% negative, 49.93% positive\n",
      "Test set has total 15883 entries with 48.84% negative, 51.16% positive\n"
     ]
    }
   ],
   "source": [
    "#See if the different test sets are evenly split between positive and negative\n",
    "\n",
    "print (\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_train),\n",
    "    (len(X_train[y_train == 0]) / (len(X_train)*1.))*100,\n",
    "    (len(X_train[y_train == 1]) / (len(X_train)*1.))*100))\n",
    "print(\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_validation),\n",
    "    (len(X_validation[y_validation == 0]) / (len(X_validation)*1.))*100,\n",
    "    (len(X_validation[y_validation == 1]) / (len(X_validation)*1.))*100))\n",
    "print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_test),\n",
    "    (len(X_test[y_test == 0]) / (len(X_test)*1.))*100,\n",
    "    (len(X_test[y_test == 1]) / (len(X_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    print(\"-\"*80)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import scipy\\nfrom scipy.sparse import hstack\\n\\nadditional_features_train = X_train[['weekday',\\n                            'hour',\\n                            'word_count',\\n                            'dot_dot_dot',\\n                            'exclamation_mark',\\n                            'question_mark',\\n                            'at_symbol',\\n                            'link',\\n                            'money',\\n                            'paragraph_symbol',\\n                            'hashtag'\\n                            ]]\\n\\n# We convert the additional_features dataframe to a sparse matrix\\nadditional_features_train_matrix = scipy.sparse.csr_matrix(additional_features_train.values)\\n\\n# Then we horizontally stack the tf-idf matrix with the additional features\\nX_train_matrix = hstack([X_train_vect_matrix, additional_features_train_matrix]) \""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add additional features to matrix build from Tweet info\n",
    "\n",
    "\"\"\" import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "additional_features_train = X_train[['weekday',\n",
    "                            'hour',\n",
    "                            'word_count',\n",
    "                            'dot_dot_dot',\n",
    "                            'exclamation_mark',\n",
    "                            'question_mark',\n",
    "                            'at_symbol',\n",
    "                            'link',\n",
    "                            'money',\n",
    "                            'paragraph_symbol',\n",
    "                            'hashtag'\n",
    "                            ]]\n",
    "\n",
    "# We convert the additional_features dataframe to a sparse matrix\n",
    "additional_features_train_matrix = scipy.sparse.csr_matrix(additional_features_train.values)\n",
    "\n",
    "# Then we horizontally stack the tf-idf matrix with the additional features\n",
    "X_train_matrix = hstack([X_train_vect_matrix, additional_features_train_matrix]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (from scipy) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 73.07%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 72.71%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 73.07%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 73.39%\n",
      "--------------------------------------------------------------------------------\n",
      "accuracy score: 73.20%\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ollin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "lr = LogisticRegression(random_state = 42)\n",
    "n_features = np.arange(100000,200001,25000)\n",
    "\n",
    "add_features_train = X_train[['weekday','hour','word_count','dot_dot_dot','exclamation_mark','question_mark','at_symbol','link','money','paragraph_symbol','hashtag']]\n",
    "add_features_validation = X_validation[['weekday','hour','word_count','dot_dot_dot','exclamation_mark','question_mark','at_symbol','link','money','paragraph_symbol','hashtag']]\n",
    "add_features_test = X_test[['weekday','hour','word_count','dot_dot_dot','exclamation_mark','question_mark','at_symbol','link','money','paragraph_symbol','hashtag']]\n",
    "\n",
    "\n",
    "for n in n_features:\n",
    "\n",
    "    #Initialise Vectoriser in loop\n",
    "    tvec = TfidfVectorizer(max_features=n,ngram_range=(1, 3))\n",
    "\n",
    "    #Vectorize String\n",
    "    X_train_matrix = tvec.fit_transform(X_train['tweet_tokenized_string'])\n",
    "    X_validation_matrix = tvec.transform(X_validation['tweet_tokenized_string'])\n",
    "    X_test_matrix = tvec.transform(X_test['tweet_tokenized_string'])\n",
    "\n",
    "    # We convert the additional_features dataframe to a sparse matrix\n",
    "    add_features_train_matrix = scipy.sparse.csr_matrix(add_features_train.values)\n",
    "    add_features_validation_matrix = scipy.sparse.csr_matrix(add_features_validation.values)\n",
    "    add_features_test_matrix = scipy.sparse.csr_matrix(add_features_test.values)\n",
    "\n",
    "    # Then we horizontally stack the tf-idf matrix with the additional features\n",
    "    X_train_matrix = hstack([X_train_matrix, add_features_train_matrix])\n",
    "    X_validation_matrix = hstack([X_validation_matrix, add_features_validation_matrix])    \n",
    "    X_test_matrix = hstack([X_test_matrix, add_features_test_matrix])\n",
    "\n",
    "    #fit(train) model\n",
    "    lr.fit(X_train_matrix, y_train)\n",
    "    y_pred = lr.predict(X_test_matrix)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    print (\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "tvec = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "\n",
    "add_features_train = X_train[['weekday','hour','word_count','dot_dot_dot','exclamation_mark','question_mark','at_symbol','link','money','paragraph_symbol','hashtag']]\n",
    "add_features_validation = X_validation[['weekday','hour','word_count','dot_dot_dot','exclamation_mark','question_mark','at_symbol','link','money','paragraph_symbol','hashtag']]\n",
    "add_features_test = X_test[['weekday','hour','word_count','dot_dot_dot','exclamation_mark','question_mark','at_symbol','link','money','paragraph_symbol','hashtag']]\n",
    "\n",
    "\n",
    "#Vectorize String\n",
    "X_train_matrix = tvec.fit_transform(X_train['tweet_tokenized_string'])\n",
    "X_validation_matrix = tvec.transform(X_validation['tweet_tokenized_string'])\n",
    "X_test_matrix = tvec.transform(X_test['tweet_tokenized_string'])\n",
    "\n",
    "# We convert the additional_features dataframe to a sparse matrix\n",
    "#add_features_train_matrix = scipy.sparse.csr_matrix(add_features_train.values)\n",
    "#add_features_validation_matrix = scipy.sparse.csr_matrix(add_features_validation.values)\n",
    "#add_features_test_matrix = scipy.sparse.csr_matrix(add_features_test.values)\n",
    "\n",
    "# Then we horizontally stack the tf-idf matrix with the additional features\n",
    "#X_train_matrix = hstack([X_train_matrix, add_features_train_matrix])\n",
    "#X_validation_matrix = hstack([X_validation_matrix, add_features_validation_matrix])    \n",
    "#X_test_matrix = hstack([X_test_matrix, add_features_test_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\ollin\\documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages (2.12.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ollin\\AppData\\Local\\Temp\\ipykernel_6740\\57882357.py:31: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nTypeError: `generator` yielded an element of shape (32,) where an element of shape (None, None) was expected.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (32,) where an element of shape (None, None) was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_3583]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\Notebooks\\olli_model_building.ipynb Cell 41\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X53sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X53sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X53sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit_generator(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X53sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     generator\u001b[39m=\u001b[39;49mbatch_generator(X_train_matrix, y_train, \u001b[39m32\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X53sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X53sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_validation_matrix, y_validation),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X53sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49mX_train_matrix\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m] \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m32\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X53sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\keras\\engine\\training.py:2636\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2624\u001b[0m \u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2625\u001b[0m \n\u001b[0;32m   2626\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2627\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2628\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2629\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2630\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2631\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2632\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2633\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2634\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2635\u001b[0m )\n\u001b[1;32m-> 2636\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   2637\u001b[0m     generator,\n\u001b[0;32m   2638\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   2639\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   2640\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2641\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   2642\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m   2643\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   2644\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[0;32m   2645\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2646\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2647\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2648\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2649\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   2650\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   2651\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nTypeError: `generator` yielded an element of shape (32,) where an element of shape (None, None) was expected.\nTraceback (most recent call last):\n\n  File \"c:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (32,) where an element of shape (None, None) was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_3583]"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#Create Generator (Split Data for training model into different batches so PC can handle it)\n",
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch // batch_size\n",
    "    counter = 0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while True:\n",
    "        index_batch = index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X_data[index_batch, :].toarray()\n",
    "        X_batch = np.expand_dims(X_batch, axis=1)  # Add an extra dimension\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if counter > number_of_batches:\n",
    "            counter = 0\n",
    "\n",
    "# Define simple Hyperparameters for model (Input Layer = 100,000, Hidden Layer = 64, Relu activation Function)\n",
    "# 1 Output Layer with Sigmoid activation Function\n",
    "# Optimization Algorithm = ADAM\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=100000))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=batch_generator(X_train_matrix, y_train, 32),\n",
    "    epochs=5,\n",
    "    validation_data=(X_validation_matrix, y_validation),\n",
    "    steps_per_epoch=X_train_matrix.shape[0] // 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.04 TiB for an array with shape (1429456, 100000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\Notebooks\\olli_model_building.ipynb Cell 42\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#Pass Sparse into dense matrix\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_train_matrix \u001b[39m=\u001b[39m X_train_matrix\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X_validation_matrix \u001b[39m=\u001b[39m X_validation_matrix\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m X_test_matrix \u001b[39m=\u001b[39m X_test_matrix\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[1;32mc:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\.conda\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.04 TiB for an array with shape (1429456, 100000) and data type float64"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Pass Sparse into dense matrix\n",
    "X_train_matrix = X_train_matrix.toarray()\n",
    "X_validation_matrix = X_validation_matrix.toarray()\n",
    "X_test_matrix = X_test_matrix.toarray()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train_matrix.shape[1]))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_matrix, y_train, epochs=5, batch_size=32, validation_data=(X_validation_matrix, y_validation))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_matrix, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "44670/44670 [==============================] - 3059s 68ms/step - loss: 0.4612 - accuracy: 0.7807 - val_loss: 0.4509 - val_accuracy: 0.7886\n",
      "Epoch 2/5\n",
      "44670/44670 [==============================] - 2978s 67ms/step - loss: 0.4273 - accuracy: 0.8025 - val_loss: 0.4528 - val_accuracy: 0.7887\n",
      "Epoch 3/5\n",
      "44670/44670 [==============================] - 2990s 67ms/step - loss: 0.4131 - accuracy: 0.8118 - val_loss: 0.4653 - val_accuracy: 0.7820\n",
      "Epoch 4/5\n",
      "44670/44670 [==============================] - 3016s 68ms/step - loss: 0.3906 - accuracy: 0.8255 - val_loss: 0.4912 - val_accuracy: 0.7723\n",
      "Epoch 5/5\n",
      "44670/44670 [==============================] - 2975s 67ms/step - loss: 0.3657 - accuracy: 0.8393 - val_loss: 0.5201 - val_accuracy: 0.7642\n",
      "496/496 [==============================] - 7s 14ms/step - loss: 0.5148 - accuracy: 0.7671\n",
      "Test Loss: 0.5148494243621826\n",
      "Test Accuracy: 0.7670741081237793\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a generator to yield batches of sparse input data\n",
    "def sparse_batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch // batch_size\n",
    "    counter = 0\n",
    "    index = np.arange(samples_per_epoch)\n",
    "    while True:\n",
    "        index_batch = index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X_data[index_batch].toarray()  # Convert SparseTensor to dense matrix\n",
    "        X_batch = np.reshape(X_batch, (X_batch.shape[0], -1))\n",
    "        y_batch = np.array(y_data.iloc[index_batch]).reshape(-1, 1)\n",
    "        counter += 1\n",
    "        yield (X_batch, y_batch)\n",
    "        if counter >= number_of_batches:\n",
    "            counter = 0\n",
    "\n",
    "\n",
    "\n",
    "# Define the architecture of your neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train_matrix.shape[1]))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the sparse batch generator\n",
    "model.fit(sparse_batch_generator(X_train_matrix, y_train, batch_size=32),\n",
    "          steps_per_epoch=X_train_matrix.shape[0] // 32,\n",
    "          epochs=5,\n",
    "          validation_data=sparse_batch_generator(X_validation_matrix, y_validation, batch_size=32),\n",
    "          validation_steps=X_validation_matrix.shape[0] // 32)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(sparse_batch_generator(X_test_matrix, y_test, batch_size=32),\n",
    "                                          steps=X_test_matrix.shape[0] // 32)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 64s 64s/step - loss: 0.6929 - accuracy: 0.5312 - val_loss: 0.6930 - val_accuracy: 0.5938\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 57s 57s/step - loss: 0.6864 - accuracy: 1.0000 - val_loss: 0.6928 - val_accuracy: 0.5625\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 58s 58s/step - loss: 0.6807 - accuracy: 1.0000 - val_loss: 0.6926 - val_accuracy: 0.5312\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 59s 59s/step - loss: 0.6749 - accuracy: 1.0000 - val_loss: 0.6924 - val_accuracy: 0.5312\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 72s 72s/step - loss: 0.6689 - accuracy: 1.0000 - val_loss: 0.6922 - val_accuracy: 0.5312\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sparse_batch_generator() missing 1 required positional argument: 'num_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ollin\\Documents\\repos\\marketing_analytics_project-1\\Notebooks\\olli_model_building.ipynb Cell 44\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m model\u001b[39m.\u001b[39mfit(sparse_batch_generator(X_train_matrix, y_train, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, num_batches\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m           steps_per_epoch\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m           epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m           validation_data\u001b[39m=\u001b[39msparse_batch_generator(X_validation_matrix, y_validation, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, num_batches\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m           validation_steps\u001b[39m=\u001b[39mX_validation_matrix\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m32\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Evaluate the model on the test data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(sparse_batch_generator(X_test_matrix, y_test, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                                           steps\u001b[39m=\u001b[39mX_test_matrix\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m32\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest Loss:\u001b[39m\u001b[39m'\u001b[39m, loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ollin/Documents/repos/marketing_analytics_project-1/Notebooks/olli_model_building.ipynb#X62sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest Accuracy:\u001b[39m\u001b[39m'\u001b[39m, accuracy)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse_batch_generator() missing 1 required positional argument: 'num_batches'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a generator to yield batches of sparse input data\n",
    "def sparse_batch_generator(X_data, y_data, batch_size, num_batches):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = min(samples_per_epoch // batch_size, num_batches)\n",
    "    counter = 0\n",
    "    index = np.arange(samples_per_epoch)\n",
    "    while counter < number_of_batches:\n",
    "        index_batch = index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X_data[index_batch].toarray()  # Convert SparseTensor to dense matrix\n",
    "        X_batch = np.reshape(X_batch, (X_batch.shape[0], -1))\n",
    "        y_batch = np.array(y_data.iloc[index_batch]).reshape(-1, 1)\n",
    "        counter += 1\n",
    "        yield (X_batch, y_batch)\n",
    "        if counter >= number_of_batches:\n",
    "            counter = 0\n",
    "\n",
    "\n",
    "# Define the architecture of your neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train_matrix.shape[1]))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the sparse batch generator\n",
    "model.fit(sparse_batch_generator(X_train_matrix, y_train, batch_size=32, num_batches=1),\n",
    "          steps_per_epoch=1,\n",
    "          epochs=3,\n",
    "          validation_data=sparse_batch_generator(X_validation_matrix, y_validation, batch_size=32, num_batches=1),\n",
    "          validation_steps=X_validation_matrix.shape[0] // 32)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(sparse_batch_generator(X_test_matrix, y_test, batch_size=32, num_batches=1),\n",
    "                                          steps=X_test_matrix.shape[0] // 32)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 [==============================] - 8s 17ms/step - loss: 0.6916 - accuracy: 0.5312\n",
      "Test Loss: 0.6915808320045471\n",
      "Test Accuracy: 0.53125\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(sparse_batch_generator(X_test_matrix, y_test, batch_size=32, num_batches=1),\n",
    "                                          steps=X_test_matrix.shape[0] // 32)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
